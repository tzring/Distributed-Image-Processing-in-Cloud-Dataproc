+ source /usr/local/share/google/dataproc/bdutil/bdutil_env.sh
++ [[ /opt/conda/default/bin:/opt/conda/miniconda3/condabin:/usr/local/sbin:/usr/local/bin:/usr/sbin:/usr/bin:/sbin:/bin == *:/snap/bin* ]]
++ PATH=/usr/local/sbin:/usr/local/bin:/usr/sbin:/usr/bin:/sbin:/bin
++ DATAPROC_DIR=/usr/local/share/google/dataproc
++ DATAPROC_TMP_DIR=/tmp/dataproc
++ DATAPROC_COMMON_LIB_DIR=/usr/local/share/google/dataproc/lib
++ DATAPROC_ARTIFACTS_DIR=/usr/local/share/google/dataproc/artifacts
++ DATAPROC_CONF_DIR=/usr/local/share/google/dataproc/conf
++ DATAPROC_METADATA_CONF=/usr/local/share/google/dataproc/conf/dataproc_metadata.sh
++ COMMANDS_TMP_DIR=/tmp/dataproc/commands
++ DATAPROC_COMPONENTS_TMP_DIR=/tmp/dataproc/components
++ COMPONENTS_TO_ACTIVATE_ENV=/usr/local/share/google/dataproc/bdutil/components/components_to_activate_env.sh
++ [[ -f /usr/local/share/google/dataproc/conf/dataproc_metadata.sh ]]
++ INSTALL_GCS_CONNECTOR=1
++ INSTALL_BIGQUERY_CONNECTOR=1
++ ENABLE_HDFS=1
++ ENABLE_HDFS_PERMISSIONS=false
++ HADOOP_INSTALL_DIR=/usr/lib/hadoop
++ HADOOP_LIB_DIR=/usr/lib/hadoop/lib
++ HADOOP_CONF_DIR=/etc/hadoop/conf
++ HADOOP_MASTER_MAPREDUCE_MEMORY_FRACTION=0.4
++ HDFS_MASTER_MEMORY_FRACTION=0.4
++ NODEMANAGER_MEMORY_FRACTION=0.8
++ NUM_WORKERS=10
++ WORKERS=()
++ CORES_PER_MAP_TASK=1.0
++ CORES_PER_REDUCE_TASK=2.0
++ CORES_PER_APP_MASTER=2.0
++ HDFS_DATA_DIRS_PERM=700
++ [[ -d /usr/lib/jvm/adoptopenjdk-8-hotspot ]]
++ [[ -d /usr/lib/jvm/adoptopenjdk-8-hotspot-amd64 ]]
++ JAVA_HOME=/usr/lib/jvm/adoptopenjdk-8-hotspot-amd64
++ SPARK_CONF_DIR=/etc/spark/conf
++ SPARK_WORKER_MEMORY_FRACTION=0.8
++ SPARK_EXECUTOR_MEMORY_FRACTION=0.8
++ SPARK_DAEMON_MEMORY_FRACTION=0.15
++ SPARK_EXECUTORS_PER_VM=2
++ TEZ_CONF_DIR=/etc/tez/conf
++ TEZ_LIB_DIR=/usr/lib/tez
+ source /usr/local/share/google/dataproc/bdutil/bdutil_helpers.sh
++ is_centos
+++ . /etc/os-release
++++ PRETTY_NAME='Debian GNU/Linux 10 (buster)'
++++ NAME='Debian GNU/Linux'
++++ VERSION_ID=10
++++ VERSION='10 (buster)'
++++ VERSION_CODENAME=buster
++++ ID=debian
++++ HOME_URL=https://www.debian.org/
++++ SUPPORT_URL=https://www.debian.org/support
++++ BUG_REPORT_URL=https://bugs.debian.org/
+++ echo debian
++ [[ debian == \c\e\n\t\o\s ]]
++ return 1
++ source /usr/local/share/google/dataproc/bdutil/os/debian/bdutil_helpers.sh
+++ APT_SENTINEL=apt.lastupdate
++ readonly EXIT_CODE_INTERNAL_ERROR=1
++ EXIT_CODE_INTERNAL_ERROR=1
++ readonly EXIT_CODE_CLIENT_ERROR=2
++ EXIT_CODE_CLIENT_ERROR=2
+ source /usr/local/share/google/dataproc/bdutil/cluster_properties.sh
+ source /usr/local/share/google/dataproc/bdutil/components/components-helpers.sh
+ source /usr/local/share/google/dataproc/bdutil/components/startup-script-components.sh
+ source /usr/local/share/google/dataproc/bdutil/components/components_to_activate_env.sh
++ set -aeuxo pipefail
++ COMPONENTS_TO_ACTIVATE='hdfs hive-metastore hive-server2 mapreduce miniconda3 mysql spark yarn'
++ HDFS_ENABLED=true
++ ROLE=Master
++ MASTER_INDEX=0
++ set +a
+ run_with_logger --tag google-dataproc-startup
+ local tag=
+ local pid=10685
+ [[ --tag == \-\-\t\a\g ]]
+ tag=google-dataproc-startup
+ shift 2
+ [[ 0 -eq 0 ]]
+ exec
++ logger -s -t 'google-dataproc-startup[10685]'
<13>Mar 20 19:55:55 google-dataproc-startup[10685]: + BACKGROUND_PROCESSES=()
<13>Mar 20 19:55:55 google-dataproc-startup[10685]: + BACKGROUND_COMMANDS=()
<13>Mar 20 19:55:55 google-dataproc-startup[10685]: + cd /tmp
<13>Mar 20 19:55:55 google-dataproc-startup[10685]: + trap logstacktrace ERR
<13>Mar 20 19:55:55 google-dataproc-startup[10685]: + loginfo 'Starting Dataproc post-hdfs startup script'
<13>Mar 20 19:55:55 google-dataproc-startup[10685]: + echo 'Starting Dataproc post-hdfs startup script'
<13>Mar 20 19:55:55 google-dataproc-startup[10685]: Starting Dataproc post-hdfs startup script
<13>Mar 20 19:55:55 google-dataproc-startup[10685]: + COMPONENTS_TO_ACTIVATE_ARRAY=(${COMPONENTS_TO_ACTIVATE})
<13>Mar 20 19:55:55 google-dataproc-startup[10685]: + post_hdfs_activate_components hdfs hive-metastore hive-server2 mapreduce miniconda3 mysql spark yarn
<13>Mar 20 19:55:55 google-dataproc-startup[10685]: + components=("$@")
<13>Mar 20 19:55:55 google-dataproc-startup[10685]: + local components
<13>Mar 20 19:55:55 google-dataproc-startup[10685]: + mkdir -p /tmp/dataproc/components/post-hdfs
<13>Mar 20 19:55:55 google-dataproc-startup[10685]: + for component in "${components[@]}"
<13>Mar 20 19:55:55 google-dataproc-startup[10685]: + loginfo 'Activating post-hdfs component hdfs'
<13>Mar 20 19:55:55 google-dataproc-startup[10685]: + echo 'Activating post-hdfs component hdfs'
<13>Mar 20 19:55:55 google-dataproc-startup[10685]: Activating post-hdfs component hdfs
<13>Mar 20 19:55:55 google-dataproc-startup[10685]: + run_in_background --tag post-hdfs-activate-component-hdfs post_hdfs_activate_component hdfs
<13>Mar 20 19:55:55 google-dataproc-startup[10685]: + local -r pid=10693
<13>Mar 20 19:55:55 google-dataproc-startup[10685]: + BACKGROUND_PROCESSES=("${pid}" "${BACKGROUND_PROCESSES[@]}")
<13>Mar 20 19:55:55 google-dataproc-startup[10685]: + [[ --tag == \-\-\t\a\g ]]
<13>Mar 20 19:55:55 google-dataproc-startup[10685]: + shift 2
<13>Mar 20 19:55:55 google-dataproc-startup[10685]: + BACKGROUND_COMMANDS=("$*" "${BACKGROUND_COMMANDS[@]}")
<13>Mar 20 19:55:55 google-dataproc-startup[10685]: + echo 'Started background process [post_hdfs_activate_component hdfs] as pid 10693'
<13>Mar 20 19:55:55 google-dataproc-startup[10685]: Started background process [post_hdfs_activate_component hdfs] as pid 10693
<13>Mar 20 19:55:55 google-dataproc-startup[10685]: + for component in "${components[@]}"
<13>Mar 20 19:55:55 google-dataproc-startup[10685]: + loginfo 'Activating post-hdfs component hive-metastore'
<13>Mar 20 19:55:55 google-dataproc-startup[10685]: + echo 'Activating post-hdfs component hive-metastore'
<13>Mar 20 19:55:55 google-dataproc-startup[10685]: Activating post-hdfs component hive-metastore
<13>Mar 20 19:55:55 google-dataproc-startup[10685]: + run_in_background --tag post-hdfs-activate-component-hive-metastore post_hdfs_activate_component hive-metastore
<13>Mar 20 19:55:55 google-dataproc-startup[10685]: + local -r pid=10694
<13>Mar 20 19:55:55 google-dataproc-startup[10685]: + BACKGROUND_PROCESSES=("${pid}" "${BACKGROUND_PROCESSES[@]}")
<13>Mar 20 19:55:55 google-dataproc-startup[10685]: + [[ --tag == \-\-\t\a\g ]]
<13>Mar 20 19:55:55 google-dataproc-startup[10685]: + shift 2
<13>Mar 20 19:55:55 google-dataproc-startup[10685]: + BACKGROUND_COMMANDS=("$*" "${BACKGROUND_COMMANDS[@]}")
<13>Mar 20 19:55:55 google-dataproc-startup[10685]: + echo 'Started background process [post_hdfs_activate_component hive-metastore] as pid 10694'
<13>Mar 20 19:55:55 google-dataproc-startup[10685]: Started background process [post_hdfs_activate_component hive-metastore] as pid 10694
<13>Mar 20 19:55:55 google-dataproc-startup[10685]: + for component in "${components[@]}"
<13>Mar 20 19:55:55 google-dataproc-startup[10685]: + loginfo 'Activating post-hdfs component hive-server2'
<13>Mar 20 19:55:55 google-dataproc-startup[10685]: + echo 'Activating post-hdfs component hive-server2'
<13>Mar 20 19:55:55 google-dataproc-startup[10685]: Activating post-hdfs component hive-server2
<13>Mar 20 19:55:55 google-dataproc-startup[10685]: + run_in_background --tag post-hdfs-activate-component-hive-server2 post_hdfs_activate_component hive-server2
<13>Mar 20 19:55:55 google-dataproc-startup[10685]: + local -r pid=10695
<13>Mar 20 19:55:55 google-dataproc-startup[10685]: + BACKGROUND_PROCESSES=("${pid}" "${BACKGROUND_PROCESSES[@]}")
<13>Mar 20 19:55:55 google-dataproc-startup[10685]: + [[ --tag == \-\-\t\a\g ]]
<13>Mar 20 19:55:55 google-dataproc-startup[10685]: + shift 2
<13>Mar 20 19:55:55 google-dataproc-startup[10685]: + BACKGROUND_COMMANDS=("$*" "${BACKGROUND_COMMANDS[@]}")
<13>Mar 20 19:55:55 google-dataproc-startup[10685]: + echo 'Started background process [post_hdfs_activate_component hive-server2] as pid 10695'
<13>Mar 20 19:55:55 google-dataproc-startup[10685]: Started background process [post_hdfs_activate_component hive-server2] as pid 10695
<13>Mar 20 19:55:55 google-dataproc-startup[10685]: + for component in "${components[@]}"
<13>Mar 20 19:55:55 google-dataproc-startup[10685]: + loginfo 'Activating post-hdfs component mapreduce'
<13>Mar 20 19:55:55 google-dataproc-startup[10685]: + echo 'Activating post-hdfs component mapreduce'
<13>Mar 20 19:55:55 google-dataproc-startup[10685]: Activating post-hdfs component mapreduce
<13>Mar 20 19:55:55 google-dataproc-startup[10685]: + run_in_background --tag post-hdfs-activate-component-mapreduce post_hdfs_activate_component mapreduce
<13>Mar 20 19:55:55 google-dataproc-startup[10685]: + local -r pid=10696
<13>Mar 20 19:55:55 google-dataproc-startup[10685]: + BACKGROUND_PROCESSES=("${pid}" "${BACKGROUND_PROCESSES[@]}")
<13>Mar 20 19:55:55 google-dataproc-startup[10685]: + [[ --tag == \-\-\t\a\g ]]
<13>Mar 20 19:55:55 google-dataproc-startup[10685]: + shift 2
<13>Mar 20 19:55:55 google-dataproc-startup[10685]: + BACKGROUND_COMMANDS=("$*" "${BACKGROUND_COMMANDS[@]}")
<13>Mar 20 19:55:55 google-dataproc-startup[10685]: + echo 'Started background process [post_hdfs_activate_component mapreduce] as pid 10696'
<13>Mar 20 19:55:55 google-dataproc-startup[10685]: Started background process [post_hdfs_activate_component mapreduce] as pid 10696
<13>Mar 20 19:55:55 google-dataproc-startup[10685]: + for component in "${components[@]}"
<13>Mar 20 19:55:55 google-dataproc-startup[10685]: + loginfo 'Activating post-hdfs component miniconda3'
<13>Mar 20 19:55:55 google-dataproc-startup[10685]: + echo 'Activating post-hdfs component miniconda3'
<13>Mar 20 19:55:55 google-dataproc-startup[10685]: Activating post-hdfs component miniconda3
<13>Mar 20 19:55:55 google-dataproc-startup[10685]: + run_in_background --tag post-hdfs-activate-component-miniconda3 post_hdfs_activate_component miniconda3
<13>Mar 20 19:55:55 google-dataproc-startup[10685]: + local -r pid=10697
<13>Mar 20 19:55:55 google-dataproc-startup[10685]: + BACKGROUND_PROCESSES=("${pid}" "${BACKGROUND_PROCESSES[@]}")
<13>Mar 20 19:55:55 google-dataproc-startup[10685]: + [[ --tag == \-\-\t\a\g ]]
<13>Mar 20 19:55:55 google-dataproc-startup[10685]: + shift 2
<13>Mar 20 19:55:55 google-dataproc-startup[10685]: + BACKGROUND_COMMANDS=("$*" "${BACKGROUND_COMMANDS[@]}")
<13>Mar 20 19:55:55 google-dataproc-startup[10685]: + echo 'Started background process [post_hdfs_activate_component miniconda3] as pid 10697'
<13>Mar 20 19:55:55 google-dataproc-startup[10685]: Started background process [post_hdfs_activate_component miniconda3] as pid 10697
<13>Mar 20 19:55:55 google-dataproc-startup[10685]: + for component in "${components[@]}"
<13>Mar 20 19:55:55 google-dataproc-startup[10685]: + loginfo 'Activating post-hdfs component mysql'
<13>Mar 20 19:55:55 google-dataproc-startup[10685]: + echo 'Activating post-hdfs component mysql'
<13>Mar 20 19:55:55 google-dataproc-startup[10685]: Activating post-hdfs component mysql
<13>Mar 20 19:55:55 google-dataproc-startup[10685]: + run_in_background --tag post-hdfs-activate-component-mysql post_hdfs_activate_component mysql
<13>Mar 20 19:55:55 google-dataproc-startup[10685]: + local -r pid=10698
<13>Mar 20 19:55:55 google-dataproc-startup[10685]: + BACKGROUND_PROCESSES=("${pid}" "${BACKGROUND_PROCESSES[@]}")
<13>Mar 20 19:55:55 google-dataproc-startup[10685]: + [[ --tag == \-\-\t\a\g ]]
<13>Mar 20 19:55:55 google-dataproc-startup[10685]: + shift 2
<13>Mar 20 19:55:55 google-dataproc-startup[10685]: + BACKGROUND_COMMANDS=("$*" "${BACKGROUND_COMMANDS[@]}")
<13>Mar 20 19:55:55 google-dataproc-startup[10685]: + echo 'Started background process [post_hdfs_activate_component mysql] as pid 10698'
<13>Mar 20 19:55:55 google-dataproc-startup[10685]: Started background process [post_hdfs_activate_component mysql] as pid 10698
<13>Mar 20 19:55:55 google-dataproc-startup[10685]: + for component in "${components[@]}"
<13>Mar 20 19:55:55 google-dataproc-startup[10685]: + loginfo 'Activating post-hdfs component spark'
<13>Mar 20 19:55:55 google-dataproc-startup[10685]: + echo 'Activating post-hdfs component spark'
<13>Mar 20 19:55:55 google-dataproc-startup[10685]: Activating post-hdfs component spark
<13>Mar 20 19:55:55 google-dataproc-startup[10685]: + run_in_background --tag post-hdfs-activate-component-spark post_hdfs_activate_component spark
<13>Mar 20 19:55:55 google-dataproc-startup[10685]: + local -r pid=10699
<13>Mar 20 19:55:55 google-dataproc-startup[10685]: + BACKGROUND_PROCESSES=("${pid}" "${BACKGROUND_PROCESSES[@]}")
<13>Mar 20 19:55:55 google-dataproc-startup[10685]: + [[ --tag == \-\-\t\a\g ]]
<13>Mar 20 19:55:55 google-dataproc-startup[10685]: + shift 2
<13>Mar 20 19:55:55 google-dataproc-startup[10685]: + BACKGROUND_COMMANDS=("$*" "${BACKGROUND_COMMANDS[@]}")
<13>Mar 20 19:55:55 google-dataproc-startup[10685]: + echo 'Started background process [post_hdfs_activate_component spark] as pid 10699'
<13>Mar 20 19:55:55 google-dataproc-startup[10685]: Started background process [post_hdfs_activate_component spark] as pid 10699
<13>Mar 20 19:55:55 google-dataproc-startup[10685]: + for component in "${components[@]}"
<13>Mar 20 19:55:55 google-dataproc-startup[10685]: + loginfo 'Activating post-hdfs component yarn'
<13>Mar 20 19:55:55 google-dataproc-startup[10685]: + echo 'Activating post-hdfs component yarn'
<13>Mar 20 19:55:55 google-dataproc-startup[10685]: Activating post-hdfs component yarn
<13>Mar 20 19:55:55 google-dataproc-startup[10685]: + run_in_background --tag post-hdfs-activate-component-yarn post_hdfs_activate_component yarn
<13>Mar 20 19:55:55 google-dataproc-startup[10685]: + local -r pid=10700
<13>Mar 20 19:55:55 google-dataproc-startup[10685]: + BACKGROUND_PROCESSES=("${pid}" "${BACKGROUND_PROCESSES[@]}")
<13>Mar 20 19:55:55 google-dataproc-startup[10685]: + [[ --tag == \-\-\t\a\g ]]
<13>Mar 20 19:55:55 google-dataproc-startup[10685]: + shift 2
<13>Mar 20 19:55:55 google-dataproc-startup[10685]: + BACKGROUND_COMMANDS=("$*" "${BACKGROUND_COMMANDS[@]}")
<13>Mar 20 19:55:55 google-dataproc-startup[10685]: + echo 'Started background process [post_hdfs_activate_component yarn] as pid 10700'
<13>Mar 20 19:55:55 google-dataproc-startup[10685]: Started background process [post_hdfs_activate_component yarn] as pid 10700
<13>Mar 20 19:55:55 google-dataproc-startup[10685]: + wait_on_async_processes
<13>Mar 20 19:55:55 google-dataproc-startup[10685]: + loginfo 'Waiting on async processes'
<13>Mar 20 19:55:55 google-dataproc-startup[10685]: + echo 'Waiting on async processes'
<13>Mar 20 19:55:55 google-dataproc-startup[10685]: Waiting on async processes
<13>Mar 20 19:55:55 google-dataproc-startup[10685]: + (( i = 0 ))
<13>Mar 20 19:55:55 google-dataproc-startup[10685]: + (( i < 8 ))
<13>Mar 20 19:55:55 google-dataproc-startup[10685]: + local pid=10700
<13>Mar 20 19:55:55 google-dataproc-startup[10685]: + local 'cmd=post_hdfs_activate_component yarn'
<13>Mar 20 19:55:55 google-dataproc-startup[10685]: + loginfo 'Waiting on pid=10700 cmd=[post_hdfs_activate_component yarn]'
<13>Mar 20 19:55:55 google-dataproc-startup[10685]: + echo 'Waiting on pid=10700 cmd=[post_hdfs_activate_component yarn]'
<13>Mar 20 19:55:55 google-dataproc-startup[10685]: Waiting on pid=10700 cmd=[post_hdfs_activate_component yarn]
<13>Mar 20 19:55:55 google-dataproc-startup[10685]: + echo 'post_hdfs_activate_component yarn'
<13>Mar 20 19:55:55 google-dataproc-startup[10685]: + local status=0
<13>Mar 20 19:55:55 google-dataproc-startup[10685]: + wait 10700
<13>Mar 20 19:55:55 google-dataproc-startup[10685]: + run_with_logger --tag post-hdfs-activate-component-spark post_hdfs_activate_component spark
<13>Mar 20 19:55:55 google-dataproc-startup[10685]: + local tag=
<13>Mar 20 19:55:55 google-dataproc-startup[10685]: + local pid=10699
<13>Mar 20 19:55:55 google-dataproc-startup[10685]: + [[ --tag == \-\-\t\a\g ]]
<13>Mar 20 19:55:55 google-dataproc-startup[10685]: + tag=post-hdfs-activate-component-spark
<13>Mar 20 19:55:55 google-dataproc-startup[10685]: + shift 2
<13>Mar 20 19:55:55 google-dataproc-startup[10685]: + [[ 2 -eq 0 ]]
<13>Mar 20 19:55:55 google-dataproc-startup[10685]: + post_hdfs_activate_component spark
<13>Mar 20 19:55:55 google-dataproc-startup[10685]: + run_with_logger --tag post-hdfs-activate-component-hive-server2 post_hdfs_activate_component hive-server2
<13>Mar 20 19:55:55 google-dataproc-startup[10685]: + local tag=
<13>Mar 20 19:55:55 google-dataproc-startup[10685]: + local pid=10695
<13>Mar 20 19:55:55 google-dataproc-startup[10685]: + [[ --tag == \-\-\t\a\g ]]
<13>Mar 20 19:55:55 google-dataproc-startup[10685]: + tag=post-hdfs-activate-component-hive-server2
<13>Mar 20 19:55:55 google-dataproc-startup[10685]: + shift 2
<13>Mar 20 19:55:55 google-dataproc-startup[10685]: + [[ 2 -eq 0 ]]
<13>Mar 20 19:55:55 google-dataproc-startup[10685]: + post_hdfs_activate_component hive-server2
<13>Mar 20 19:55:55 google-dataproc-startup[10685]: + run_with_logger --tag post-hdfs-activate-component-miniconda3 post_hdfs_activate_component miniconda3
<13>Mar 20 19:55:55 google-dataproc-startup[10685]: + local tag=
<13>Mar 20 19:55:55 google-dataproc-startup[10685]: + local pid=10697
<13>Mar 20 19:55:55 google-dataproc-startup[10685]: + [[ --tag == \-\-\t\a\g ]]
<13>Mar 20 19:55:55 google-dataproc-startup[10685]: + tag=post-hdfs-activate-component-miniconda3
<13>Mar 20 19:55:55 google-dataproc-startup[10685]: + shift 2
<13>Mar 20 19:55:55 google-dataproc-startup[10685]: + [[ 2 -eq 0 ]]
<13>Mar 20 19:55:55 google-dataproc-startup[10685]: + post_hdfs_activate_component miniconda3
<13>Mar 20 19:55:55 google-dataproc-startup[10685]: + run_with_logger --tag post-hdfs-activate-component-mapreduce post_hdfs_activate_component mapreduce
<13>Mar 20 19:55:55 google-dataproc-startup[10685]: + local tag=
<13>Mar 20 19:55:55 google-dataproc-startup[10685]: + local pid=10696
<13>Mar 20 19:55:55 google-dataproc-startup[10685]: + [[ --tag == \-\-\t\a\g ]]
<13>Mar 20 19:55:55 google-dataproc-startup[10685]: + tag=post-hdfs-activate-component-mapreduce
<13>Mar 20 19:55:55 google-dataproc-startup[10685]: + shift 2
<13>Mar 20 19:55:55 google-dataproc-startup[10685]: + [[ 2 -eq 0 ]]
<13>Mar 20 19:55:55 google-dataproc-startup[10685]: + post_hdfs_activate_component mapreduce
<13>Mar 20 19:55:55 google-dataproc-startup[10685]: + run_with_logger --tag post-hdfs-activate-component-hive-metastore post_hdfs_activate_component hive-metastore
<13>Mar 20 19:55:55 google-dataproc-startup[10685]: + local tag=
<13>Mar 20 19:55:55 google-dataproc-startup[10685]: + local pid=10694
<13>Mar 20 19:55:55 google-dataproc-startup[10685]: + [[ --tag == \-\-\t\a\g ]]
<13>Mar 20 19:55:55 google-dataproc-startup[10685]: + tag=post-hdfs-activate-component-hive-metastore
<13>Mar 20 19:55:55 google-dataproc-startup[10685]: + shift 2
<13>Mar 20 19:55:55 google-dataproc-startup[10685]: + [[ 2 -eq 0 ]]
<13>Mar 20 19:55:55 google-dataproc-startup[10685]: + post_hdfs_activate_component hive-metastore
<13>Mar 20 19:55:55 google-dataproc-startup[10685]: ++ logger -s -t 'post-hdfs-activate-component-spark[10699]'
<13>Mar 20 19:55:55 google-dataproc-startup[10685]: ++ logger -s -t 'post-hdfs-activate-component-miniconda3[10697]'
<13>Mar 20 19:55:55 google-dataproc-startup[10685]: + run_with_logger --tag post-hdfs-activate-component-hdfs post_hdfs_activate_component hdfs
<13>Mar 20 19:55:55 google-dataproc-startup[10685]: + local tag=
<13>Mar 20 19:55:55 google-dataproc-startup[10685]: + local pid=10693
<13>Mar 20 19:55:55 google-dataproc-startup[10685]: + [[ --tag == \-\-\t\a\g ]]
<13>Mar 20 19:55:55 google-dataproc-startup[10685]: + tag=post-hdfs-activate-component-hdfs
<13>Mar 20 19:55:55 google-dataproc-startup[10685]: + shift 2
<13>Mar 20 19:55:55 google-dataproc-startup[10685]: + [[ 2 -eq 0 ]]
<13>Mar 20 19:55:55 google-dataproc-startup[10685]: + post_hdfs_activate_component hdfs
<13>Mar 20 19:55:55 google-dataproc-startup[10685]: ++ logger -s -t 'post-hdfs-activate-component-hive-metastore[10694]'
<13>Mar 20 19:55:55 google-dataproc-startup[10685]: + run_with_logger --tag post-hdfs-activate-component-mysql post_hdfs_activate_component mysql
<13>Mar 20 19:55:55 google-dataproc-startup[10685]: + local tag=
<13>Mar 20 19:55:55 google-dataproc-startup[10685]: + local pid=10698
<13>Mar 20 19:55:55 google-dataproc-startup[10685]: + [[ --tag == \-\-\t\a\g ]]
<13>Mar 20 19:55:55 google-dataproc-startup[10685]: + tag=post-hdfs-activate-component-mysql
<13>Mar 20 19:55:55 google-dataproc-startup[10685]: + shift 2
<13>Mar 20 19:55:55 google-dataproc-startup[10685]: + [[ 2 -eq 0 ]]
<13>Mar 20 19:55:55 google-dataproc-startup[10685]: + post_hdfs_activate_component mysql
<13>Mar 20 19:55:55 google-dataproc-startup[10685]: <13>Mar 20 19:55:55 post-hdfs-activate-component-spark[10699]: + local -r component=spark
<13>Mar 20 19:55:55 google-dataproc-startup[10685]: <13>Mar 20 19:55:55 post-hdfs-activate-component-spark[10699]: + local -r post_hdfs_script=/usr/local/share/google/dataproc/bdutil/components/post-hdfs/spark.sh
<13>Mar 20 19:55:55 google-dataproc-startup[10685]: <13>Mar 20 19:55:55 post-hdfs-activate-component-spark[10699]: + [[ -f /usr/local/share/google/dataproc/bdutil/components/post-hdfs/spark.sh ]]
<13>Mar 20 19:55:55 google-dataproc-startup[10685]: <13>Mar 20 19:55:55 post-hdfs-activate-component-spark[10699]: + echo 'Running component activate post-hdfs script: /usr/local/share/google/dataproc/bdutil/components/post-hdfs/spark.sh'
<13>Mar 20 19:55:55 google-dataproc-startup[10685]: <13>Mar 20 19:55:55 post-hdfs-activate-component-spark[10699]: Running component activate post-hdfs script: /usr/local/share/google/dataproc/bdutil/components/post-hdfs/spark.sh
<13>Mar 20 19:55:55 google-dataproc-startup[10685]: <13>Mar 20 19:55:55 post-hdfs-activate-component-spark[10699]: + touch /tmp/dataproc/components/post-hdfs/spark.running
<13>Mar 20 19:55:55 google-dataproc-startup[10685]: <13>Mar 20 19:55:55 post-hdfs-activate-component-spark[10699]: + local exit_code=0
<13>Mar 20 19:55:55 google-dataproc-startup[10685]: <13>Mar 20 19:55:55 post-hdfs-activate-component-spark[10699]: + bash -ex /usr/local/share/google/dataproc/bdutil/components/post-hdfs/spark.sh
<13>Mar 20 19:55:55 google-dataproc-startup[10685]: ++ logger -s -t 'post-hdfs-activate-component-hdfs[10693]'
<13>Mar 20 19:55:55 google-dataproc-startup[10685]: <13>Mar 20 19:55:55 post-hdfs-activate-component-hive-metastore[10694]: + local -r component=hive-metastore
<13>Mar 20 19:55:55 google-dataproc-startup[10685]: <13>Mar 20 19:55:55 post-hdfs-activate-component-hive-metastore[10694]: + local -r post_hdfs_script=/usr/local/share/google/dataproc/bdutil/components/post-hdfs/hive-metastore.sh
<13>Mar 20 19:55:55 google-dataproc-startup[10685]: <13>Mar 20 19:55:55 post-hdfs-activate-component-hive-metastore[10694]: + [[ -f /usr/local/share/google/dataproc/bdutil/components/post-hdfs/hive-metastore.sh ]]
<13>Mar 20 19:55:55 google-dataproc-startup[10685]: <13>Mar 20 19:55:55 post-hdfs-activate-component-hive-metastore[10694]: + echo 'Component hive-metastore doesn'\''t have a post-hdfs script'
<13>Mar 20 19:55:55 google-dataproc-startup[10685]: <13>Mar 20 19:55:55 post-hdfs-activate-component-hive-metastore[10694]: Component hive-metastore doesn't have a post-hdfs script
<13>Mar 20 19:55:55 google-dataproc-startup[10685]: <13>Mar 20 19:55:55 post-hdfs-activate-component-spark[10699]: + set -euxo pipefail
<13>Mar 20 19:55:55 google-dataproc-startup[10685]: + run_with_logger --tag post-hdfs-activate-component-yarn post_hdfs_activate_component yarn
<13>Mar 20 19:55:55 google-dataproc-startup[10685]: <13>Mar 20 19:55:55 post-hdfs-activate-component-hdfs[10693]: + local -r component=hdfs
<13>Mar 20 19:55:55 google-dataproc-startup[10685]: <13>Mar 20 19:55:55 post-hdfs-activate-component-hdfs[10693]: + local -r post_hdfs_script=/usr/local/share/google/dataproc/bdutil/components/post-hdfs/hdfs.sh
<13>Mar 20 19:55:55 google-dataproc-startup[10685]: + local tag=
<13>Mar 20 19:55:55 google-dataproc-startup[10685]: <13>Mar 20 19:55:55 post-hdfs-activate-component-hdfs[10693]: + [[ -f /usr/local/share/google/dataproc/bdutil/components/post-hdfs/hdfs.sh ]]
<13>Mar 20 19:55:55 google-dataproc-startup[10685]: <13>Mar 20 19:55:55 post-hdfs-activate-component-hdfs[10693]: + echo 'Component hdfs doesn'\''t have a post-hdfs script'
<13>Mar 20 19:55:55 google-dataproc-startup[10685]: <13>Mar 20 19:55:55 post-hdfs-activate-component-hdfs[10693]: Component hdfs doesn't have a post-hdfs script
<13>Mar 20 19:55:55 google-dataproc-startup[10685]: + local pid=10700
<13>Mar 20 19:55:55 google-dataproc-startup[10685]: + [[ --tag == \-\-\t\a\g ]]
<13>Mar 20 19:55:55 google-dataproc-startup[10685]: + tag=post-hdfs-activate-component-yarn
<13>Mar 20 19:55:55 google-dataproc-startup[10685]: + shift 2
<13>Mar 20 19:55:55 google-dataproc-startup[10685]: + [[ 2 -eq 0 ]]
<13>Mar 20 19:55:55 google-dataproc-startup[10685]: + post_hdfs_activate_component yarn
<13>Mar 20 19:55:55 google-dataproc-startup[10685]: <13>Mar 20 19:55:55 post-hdfs-activate-component-spark[10699]: ++ dirname /usr/local/share/google/dataproc/bdutil/components/post-hdfs/spark.sh
<13>Mar 20 19:55:55 google-dataproc-startup[10685]: + (( status != 0 ))
<13>Mar 20 19:55:55 google-dataproc-startup[10685]: <13>Mar 20 19:55:55 post-hdfs-activate-component-spark[10699]: + source /usr/local/share/google/dataproc/bdutil/components/post-hdfs/../../bdutil_env.sh
<13>Mar 20 19:55:55 google-dataproc-startup[10685]: <13>Mar 20 19:55:55 post-hdfs-activate-component-spark[10699]: ++ [[ /usr/local/sbin:/usr/local/bin:/usr/sbin:/usr/bin:/sbin:/bin == *:/snap/bin* ]]
<13>Mar 20 19:55:55 google-dataproc-startup[10685]: <13>Mar 20 19:55:55 post-hdfs-activate-component-spark[10699]: ++ PATH=/usr/local/sbin:/usr/local/bin:/usr/sbin:/usr/bin:/sbin:/bin
<13>Mar 20 19:55:55 google-dataproc-startup[10685]: <13>Mar 20 19:55:55 post-hdfs-activate-component-spark[10699]: ++ DATAPROC_DIR=/usr/local/share/google/dataproc
<13>Mar 20 19:55:55 google-dataproc-startup[10685]: <13>Mar 20 19:55:55 post-hdfs-activate-component-spark[10699]: ++ DATAPROC_TMP_DIR=/tmp/dataproc
<13>Mar 20 19:55:55 google-dataproc-startup[10685]: <13>Mar 20 19:55:55 post-hdfs-activate-component-spark[10699]: ++ DATAPROC_COMMON_LIB_DIR=/usr/local/share/google/dataproc/lib
<13>Mar 20 19:55:55 google-dataproc-startup[10685]: <13>Mar 20 19:55:55 post-hdfs-activate-component-spark[10699]: ++ DATAPROC_ARTIFACTS_DIR=/usr/local/share/google/dataproc/artifacts
<13>Mar 20 19:55:55 google-dataproc-startup[10685]: <13>Mar 20 19:55:55 post-hdfs-activate-component-spark[10699]: ++ DATAPROC_CONF_DIR=/usr/local/share/google/dataproc/conf
<13>Mar 20 19:55:55 google-dataproc-startup[10685]: <13>Mar 20 19:55:55 post-hdfs-activate-component-spark[10699]: ++ DATAPROC_METADATA_CONF=/usr/local/share/google/dataproc/conf/dataproc_metadata.sh
<13>Mar 20 19:55:55 google-dataproc-startup[10685]: <13>Mar 20 19:55:55 post-hdfs-activate-component-spark[10699]: ++ COMMANDS_TMP_DIR=/tmp/dataproc/commands
<13>Mar 20 19:55:55 google-dataproc-startup[10685]: <13>Mar 20 19:55:55 post-hdfs-activate-component-spark[10699]: ++ DATAPROC_COMPONENTS_TMP_DIR=/tmp/dataproc/components
<13>Mar 20 19:55:55 google-dataproc-startup[10685]: <13>Mar 20 19:55:55 post-hdfs-activate-component-spark[10699]: ++ COMPONENTS_TO_ACTIVATE_ENV=/usr/local/share/google/dataproc/bdutil/components/components_to_activate_env.sh
<13>Mar 20 19:55:55 google-dataproc-startup[10685]: <13>Mar 20 19:55:55 post-hdfs-activate-component-spark[10699]: ++ [[ -f /usr/local/share/google/dataproc/conf/dataproc_metadata.sh ]]
<13>Mar 20 19:55:55 google-dataproc-startup[10685]: <13>Mar 20 19:55:55 post-hdfs-activate-component-spark[10699]: ++ INSTALL_GCS_CONNECTOR=1
<13>Mar 20 19:55:55 google-dataproc-startup[10685]: <13>Mar 20 19:55:55 post-hdfs-activate-component-spark[10699]: ++ INSTALL_BIGQUERY_CONNECTOR=1
<13>Mar 20 19:55:55 google-dataproc-startup[10685]: <13>Mar 20 19:55:55 post-hdfs-activate-component-spark[10699]: ++ ENABLE_HDFS=1
<13>Mar 20 19:55:55 google-dataproc-startup[10685]: <13>Mar 20 19:55:55 post-hdfs-activate-component-spark[10699]: ++ ENABLE_HDFS_PERMISSIONS=false
<13>Mar 20 19:55:55 google-dataproc-startup[10685]: <13>Mar 20 19:55:55 post-hdfs-activate-component-spark[10699]: ++ HADOOP_INSTALL_DIR=/usr/lib/hadoop
<13>Mar 20 19:55:55 google-dataproc-startup[10685]: <13>Mar 20 19:55:55 post-hdfs-activate-component-spark[10699]: ++ HADOOP_LIB_DIR=/usr/lib/hadoop/lib
<13>Mar 20 19:55:55 google-dataproc-startup[10685]: <13>Mar 20 19:55:55 post-hdfs-activate-component-spark[10699]: ++ HADOOP_CONF_DIR=/etc/hadoop/conf
<13>Mar 20 19:55:55 google-dataproc-startup[10685]: <13>Mar 20 19:55:55 post-hdfs-activate-component-spark[10699]: ++ HADOOP_MASTER_MAPREDUCE_MEMORY_FRACTION=0.4
<13>Mar 20 19:55:55 google-dataproc-startup[10685]: <13>Mar 20 19:55:55 post-hdfs-activate-component-spark[10699]: ++ HDFS_MASTER_MEMORY_FRACTION=0.4
<13>Mar 20 19:55:55 google-dataproc-startup[10685]: <13>Mar 20 19:55:55 post-hdfs-activate-component-spark[10699]: ++ NODEMANAGER_MEMORY_FRACTION=0.8
<13>Mar 20 19:55:55 google-dataproc-startup[10685]: <13>Mar 20 19:55:55 post-hdfs-activate-component-spark[10699]: ++ NUM_WORKERS=10
<13>Mar 20 19:55:55 google-dataproc-startup[10685]: <13>Mar 20 19:55:55 post-hdfs-activate-component-spark[10699]: ++ WORKERS=()
<13>Mar 20 19:55:55 google-dataproc-startup[10685]: <13>Mar 20 19:55:55 post-hdfs-activate-component-spark[10699]: ++ CORES_PER_MAP_TASK=1.0
<13>Mar 20 19:55:55 google-dataproc-startup[10685]: <13>Mar 20 19:55:55 post-hdfs-activate-component-spark[10699]: ++ CORES_PER_REDUCE_TASK=2.0
<13>Mar 20 19:55:55 google-dataproc-startup[10685]: <13>Mar 20 19:55:55 post-hdfs-activate-component-spark[10699]: ++ CORES_PER_APP_MASTER=2.0
<13>Mar 20 19:55:55 google-dataproc-startup[10685]: <13>Mar 20 19:55:55 post-hdfs-activate-component-spark[10699]: ++ HDFS_DATA_DIRS_PERM=700
<13>Mar 20 19:55:55 google-dataproc-startup[10685]: <13>Mar 20 19:55:55 post-hdfs-activate-component-spark[10699]: ++ [[ -d /usr/lib/jvm/adoptopenjdk-8-hotspot ]]
<13>Mar 20 19:55:55 google-dataproc-startup[10685]: <13>Mar 20 19:55:55 post-hdfs-activate-component-spark[10699]: ++ [[ -d /usr/lib/jvm/adoptopenjdk-8-hotspot-amd64 ]]
<13>Mar 20 19:55:55 google-dataproc-startup[10685]: <13>Mar 20 19:55:55 post-hdfs-activate-component-spark[10699]: ++ JAVA_HOME=/usr/lib/jvm/adoptopenjdk-8-hotspot-amd64
<13>Mar 20 19:55:55 google-dataproc-startup[10685]: <13>Mar 20 19:55:55 post-hdfs-activate-component-spark[10699]: ++ SPARK_CONF_DIR=/etc/spark/conf
<13>Mar 20 19:55:55 google-dataproc-startup[10685]: <13>Mar 20 19:55:55 post-hdfs-activate-component-spark[10699]: ++ SPARK_WORKER_MEMORY_FRACTION=0.8
<13>Mar 20 19:55:55 google-dataproc-startup[10685]: <13>Mar 20 19:55:55 post-hdfs-activate-component-spark[10699]: ++ SPARK_EXECUTOR_MEMORY_FRACTION=0.8
<13>Mar 20 19:55:55 google-dataproc-startup[10685]: <13>Mar 20 19:55:55 post-hdfs-activate-component-spark[10699]: ++ SPARK_DAEMON_MEMORY_FRACTION=0.15
<13>Mar 20 19:55:55 google-dataproc-startup[10685]: <13>Mar 20 19:55:55 post-hdfs-activate-component-spark[10699]: ++ SPARK_EXECUTORS_PER_VM=2
<13>Mar 20 19:55:55 google-dataproc-startup[10685]: <13>Mar 20 19:55:55 post-hdfs-activate-component-spark[10699]: ++ TEZ_CONF_DIR=/etc/tez/conf
<13>Mar 20 19:55:55 google-dataproc-startup[10685]: <13>Mar 20 19:55:55 post-hdfs-activate-component-spark[10699]: ++ TEZ_LIB_DIR=/usr/lib/tez
<13>Mar 20 19:55:55 google-dataproc-startup[10685]: ++ logger -s -t 'post-hdfs-activate-component-yarn[10700]'
<13>Mar 20 19:55:55 google-dataproc-startup[10685]: + tee /tmp/dataproc/commands/10700.done
<13>Mar 20 19:55:55 google-dataproc-startup[10685]: <13>Mar 20 19:55:55 post-hdfs-activate-component-spark[10699]: ++ dirname /usr/local/share/google/dataproc/bdutil/components/post-hdfs/spark.sh
<13>Mar 20 19:55:55 google-dataproc-startup[10685]: + echo 'Command cmd=[post_hdfs_activate_component yarn] pid=10700 exited with 0'
<13>Mar 20 19:55:55 google-dataproc-startup[10685]: Command cmd=[post_hdfs_activate_component yarn] pid=10700 exited with 0
<13>Mar 20 19:55:55 google-dataproc-startup[10685]: + (( ++i ))
<13>Mar 20 19:55:55 google-dataproc-startup[10685]: + (( i < 8 ))
<13>Mar 20 19:55:55 google-dataproc-startup[10685]: + local pid=10699
<13>Mar 20 19:55:55 google-dataproc-startup[10685]: + local 'cmd=post_hdfs_activate_component spark'
<13>Mar 20 19:55:55 google-dataproc-startup[10685]: + loginfo 'Waiting on pid=10699 cmd=[post_hdfs_activate_component spark]'
<13>Mar 20 19:55:55 google-dataproc-startup[10685]: + echo 'Waiting on pid=10699 cmd=[post_hdfs_activate_component spark]'
<13>Mar 20 19:55:55 google-dataproc-startup[10685]: Waiting on pid=10699 cmd=[post_hdfs_activate_component spark]
<13>Mar 20 19:55:55 google-dataproc-startup[10685]: + echo 'post_hdfs_activate_component spark'
<13>Mar 20 19:55:55 google-dataproc-startup[10685]: + local status=0
<13>Mar 20 19:55:55 google-dataproc-startup[10685]: + wait 10699
<13>Mar 20 19:55:55 google-dataproc-startup[10685]: <13>Mar 20 19:55:55 post-hdfs-activate-component-spark[10699]: + source /usr/local/share/google/dataproc/bdutil/components/post-hdfs/../../bdutil_helpers.sh
<13>Mar 20 19:55:55 google-dataproc-startup[10685]: <13>Mar 20 19:55:55 post-hdfs-activate-component-spark[10699]: ++ is_centos
<13>Mar 20 19:55:55 google-dataproc-startup[10685]: <13>Mar 20 19:55:55 post-hdfs-activate-component-spark[10699]: +++ . /etc/os-release
<13>Mar 20 19:55:55 google-dataproc-startup[10685]: <13>Mar 20 19:55:55 post-hdfs-activate-component-spark[10699]: ++++ PRETTY_NAME='Debian GNU/Linux 10 (buster)'
<13>Mar 20 19:55:55 google-dataproc-startup[10685]: <13>Mar 20 19:55:55 post-hdfs-activate-component-spark[10699]: ++++ NAME='Debian GNU/Linux'
<13>Mar 20 19:55:55 google-dataproc-startup[10685]: <13>Mar 20 19:55:55 post-hdfs-activate-component-spark[10699]: ++++ VERSION_ID=10
<13>Mar 20 19:55:55 google-dataproc-startup[10685]: <13>Mar 20 19:55:55 post-hdfs-activate-component-spark[10699]: ++++ VERSION='10 (buster)'
<13>Mar 20 19:55:55 google-dataproc-startup[10685]: <13>Mar 20 19:55:55 post-hdfs-activate-component-spark[10699]: ++++ VERSION_CODENAME=buster
<13>Mar 20 19:55:55 google-dataproc-startup[10685]: <13>Mar 20 19:55:55 post-hdfs-activate-component-spark[10699]: ++++ ID=debian
<13>Mar 20 19:55:55 google-dataproc-startup[10685]: <13>Mar 20 19:55:55 post-hdfs-activate-component-spark[10699]: ++++ HOME_URL=https://www.debian.org/
<13>Mar 20 19:55:55 google-dataproc-startup[10685]: <13>Mar 20 19:55:55 post-hdfs-activate-component-spark[10699]: ++++ SUPPORT_URL=https://www.debian.org/support
<13>Mar 20 19:55:55 google-dataproc-startup[10685]: <13>Mar 20 19:55:55 post-hdfs-activate-component-spark[10699]: ++++ BUG_REPORT_URL=https://bugs.debian.org/
<13>Mar 20 19:55:55 google-dataproc-startup[10685]: <13>Mar 20 19:55:55 post-hdfs-activate-component-spark[10699]: +++ echo debian
<13>Mar 20 19:55:55 google-dataproc-startup[10685]: <13>Mar 20 19:55:55 post-hdfs-activate-component-spark[10699]: ++ [[ debian == \c\e\n\t\o\s ]]
<13>Mar 20 19:55:55 google-dataproc-startup[10685]: <13>Mar 20 19:55:55 post-hdfs-activate-component-spark[10699]: ++ return 1
<13>Mar 20 19:55:55 google-dataproc-startup[10685]: <13>Mar 20 19:55:55 post-hdfs-activate-component-spark[10699]: ++ source /usr/local/share/google/dataproc/bdutil/os/debian/bdutil_helpers.sh
<13>Mar 20 19:55:55 google-dataproc-startup[10685]: <13>Mar 20 19:55:55 post-hdfs-activate-component-spark[10699]: +++ APT_SENTINEL=apt.lastupdate
<13>Mar 20 19:55:55 google-dataproc-startup[10685]: <13>Mar 20 19:55:55 post-hdfs-activate-component-spark[10699]: ++ readonly EXIT_CODE_INTERNAL_ERROR=1
<13>Mar 20 19:55:55 google-dataproc-startup[10685]: <13>Mar 20 19:55:55 post-hdfs-activate-component-spark[10699]: ++ EXIT_CODE_INTERNAL_ERROR=1
<13>Mar 20 19:55:55 google-dataproc-startup[10685]: <13>Mar 20 19:55:55 post-hdfs-activate-component-spark[10699]: ++ readonly EXIT_CODE_CLIENT_ERROR=2
<13>Mar 20 19:55:55 google-dataproc-startup[10685]: <13>Mar 20 19:55:55 post-hdfs-activate-component-spark[10699]: ++ EXIT_CODE_CLIENT_ERROR=2
<13>Mar 20 19:55:55 google-dataproc-startup[10685]: <13>Mar 20 19:55:55 post-hdfs-activate-component-spark[10699]: ++ dirname /usr/local/share/google/dataproc/bdutil/components/post-hdfs/spark.sh
<13>Mar 20 19:55:55 google-dataproc-startup[10685]: <13>Mar 20 19:55:55 post-hdfs-activate-component-spark[10699]: + source /usr/local/share/google/dataproc/bdutil/components/post-hdfs/../../cluster_properties.sh
<13>Mar 20 19:55:55 google-dataproc-startup[10685]: <13>Mar 20 19:55:55 post-hdfs-activate-component-spark[10699]: ++ dirname /usr/local/share/google/dataproc/bdutil/components/post-hdfs/spark.sh
<13>Mar 20 19:55:55 google-dataproc-startup[10685]: <13>Mar 20 19:55:55 post-hdfs-activate-component-spark[10699]: + source /usr/local/share/google/dataproc/bdutil/components/post-hdfs/../shared/spark.sh
<13>Mar 20 19:55:55 google-dataproc-startup[10685]: <13>Mar 20 19:55:55 post-hdfs-activate-component-spark[10699]: + set_log_tag post-hdfs-component-spark
<13>Mar 20 19:55:55 google-dataproc-startup[10685]: <13>Mar 20 19:55:55 post-hdfs-activate-component-spark[10699]: + local -r tag=post-hdfs-component-spark
<13>Mar 20 19:55:55 google-dataproc-startup[10685]: <13>Mar 20 19:55:55 post-hdfs-activate-component-spark[10699]: + exec
<13>Mar 20 19:55:55 google-dataproc-startup[10685]: ++ logger -s -t 'post-hdfs-activate-component-hive-server2[10695]'
<13>Mar 20 19:55:55 google-dataproc-startup[10685]: <13>Mar 20 19:55:55 post-hdfs-activate-component-spark[10699]: ++ logger -s -t 'post-hdfs-component-spark[10724]'
<13>Mar 20 19:55:55 google-dataproc-startup[10685]: <13>Mar 20 19:55:55 post-hdfs-activate-component-hive-server2[10695]: + local -r component=hive-server2
<13>Mar 20 19:55:55 google-dataproc-startup[10685]: <13>Mar 20 19:55:55 post-hdfs-activate-component-hive-server2[10695]: + local -r post_hdfs_script=/usr/local/share/google/dataproc/bdutil/components/post-hdfs/hive-server2.sh
<13>Mar 20 19:55:55 google-dataproc-startup[10685]: <13>Mar 20 19:55:55 post-hdfs-activate-component-hive-server2[10695]: + [[ -f /usr/local/share/google/dataproc/bdutil/components/post-hdfs/hive-server2.sh ]]
<13>Mar 20 19:55:55 google-dataproc-startup[10685]: <13>Mar 20 19:55:55 post-hdfs-activate-component-hive-server2[10695]: + echo 'Running component activate post-hdfs script: /usr/local/share/google/dataproc/bdutil/components/post-hdfs/hive-server2.sh'
<13>Mar 20 19:55:55 google-dataproc-startup[10685]: <13>Mar 20 19:55:55 post-hdfs-activate-component-hive-server2[10695]: Running component activate post-hdfs script: /usr/local/share/google/dataproc/bdutil/components/post-hdfs/hive-server2.sh
<13>Mar 20 19:55:55 google-dataproc-startup[10685]: <13>Mar 20 19:55:55 post-hdfs-activate-component-hive-server2[10695]: + touch /tmp/dataproc/components/post-hdfs/hive-server2.running
<13>Mar 20 19:55:55 google-dataproc-startup[10685]: <13>Mar 20 19:55:55 post-hdfs-activate-component-hive-server2[10695]: + local exit_code=0
<13>Mar 20 19:55:55 google-dataproc-startup[10685]: <13>Mar 20 19:55:55 post-hdfs-activate-component-hive-server2[10695]: + bash -ex /usr/local/share/google/dataproc/bdutil/components/post-hdfs/hive-server2.sh
<13>Mar 20 19:55:55 google-dataproc-startup[10685]: ++ logger -s -t 'post-hdfs-activate-component-mysql[10698]'
<13>Mar 20 19:55:55 google-dataproc-startup[10685]: ++ logger -s -t 'post-hdfs-activate-component-mapreduce[10696]'
<13>Mar 20 19:55:55 google-dataproc-startup[10685]: <13>Mar 20 19:55:55 post-hdfs-activate-component-miniconda3[10697]: + local -r component=miniconda3
<13>Mar 20 19:55:55 google-dataproc-startup[10685]: <13>Mar 20 19:55:55 post-hdfs-activate-component-miniconda3[10697]: + local -r post_hdfs_script=/usr/local/share/google/dataproc/bdutil/components/post-hdfs/miniconda3.sh
<13>Mar 20 19:55:55 google-dataproc-startup[10685]: <13>Mar 20 19:55:55 post-hdfs-activate-component-miniconda3[10697]: + [[ -f /usr/local/share/google/dataproc/bdutil/components/post-hdfs/miniconda3.sh ]]
<13>Mar 20 19:55:55 google-dataproc-startup[10685]: <13>Mar 20 19:55:55 post-hdfs-activate-component-miniconda3[10697]: + echo 'Component miniconda3 doesn'\''t have a post-hdfs script'
<13>Mar 20 19:55:55 google-dataproc-startup[10685]: <13>Mar 20 19:55:55 post-hdfs-activate-component-miniconda3[10697]: Component miniconda3 doesn't have a post-hdfs script
<13>Mar 20 19:55:55 google-dataproc-startup[10685]: <13>Mar 20 19:55:55 post-hdfs-activate-component-spark[10699]: <13>Mar 20 19:55:55 post-hdfs-component-spark[10724]: + [[ true == \t\r\u\e ]]
<13>Mar 20 19:55:55 google-dataproc-startup[10685]: <13>Mar 20 19:55:55 post-hdfs-activate-component-spark[10699]: <13>Mar 20 19:55:55 post-hdfs-component-spark[10724]: + [[ Master == \M\a\s\t\e\r ]]
<13>Mar 20 19:55:55 google-dataproc-startup[10685]: <13>Mar 20 19:55:55 post-hdfs-activate-component-spark[10699]: <13>Mar 20 19:55:55 post-hdfs-component-spark[10724]: + [[ 0 == \0 ]]
<13>Mar 20 19:55:55 google-dataproc-startup[10685]: <13>Mar 20 19:55:55 post-hdfs-activate-component-spark[10699]: <13>Mar 20 19:55:55 post-hdfs-component-spark[10724]: + start_spark_history_server
<13>Mar 20 19:55:55 google-dataproc-startup[10685]: <13>Mar 20 19:55:55 post-hdfs-activate-component-spark[10699]: <13>Mar 20 19:55:55 post-hdfs-component-spark[10724]: + enable_service spark-history-server
<13>Mar 20 19:55:55 google-dataproc-startup[10685]: <13>Mar 20 19:55:55 post-hdfs-activate-component-spark[10699]: <13>Mar 20 19:55:55 post-hdfs-component-spark[10724]: + local -r service=spark-history-server
<13>Mar 20 19:55:55 google-dataproc-startup[10685]: <13>Mar 20 19:55:55 post-hdfs-activate-component-spark[10699]: <13>Mar 20 19:55:55 post-hdfs-component-spark[10724]: + local -r unit=spark-history-server.service
<13>Mar 20 19:55:55 google-dataproc-startup[10685]: <13>Mar 20 19:55:55 post-hdfs-activate-component-spark[10699]: <13>Mar 20 19:55:55 post-hdfs-component-spark[10724]: + retry_constant_short systemctl enable spark-history-server.service
<13>Mar 20 19:55:55 google-dataproc-startup[10685]: <13>Mar 20 19:55:55 post-hdfs-activate-component-spark[10699]: <13>Mar 20 19:55:55 post-hdfs-component-spark[10724]: + retry_constant_custom 30 1 systemctl enable spark-history-server.service
<13>Mar 20 19:55:55 google-dataproc-startup[10685]: <13>Mar 20 19:55:55 post-hdfs-activate-component-spark[10699]: <13>Mar 20 19:55:55 post-hdfs-component-spark[10724]: + local -r max_retry_time=30
<13>Mar 20 19:55:55 google-dataproc-startup[10685]: <13>Mar 20 19:55:55 post-hdfs-activate-component-spark[10699]: <13>Mar 20 19:55:55 post-hdfs-component-spark[10724]: + local -r retry_delay=1
<13>Mar 20 19:55:55 google-dataproc-startup[10685]: <13>Mar 20 19:55:55 post-hdfs-activate-component-spark[10699]: <13>Mar 20 19:55:55 post-hdfs-component-spark[10724]: + cmd=("${@:3}")
<13>Mar 20 19:55:55 google-dataproc-startup[10685]: <13>Mar 20 19:55:55 post-hdfs-activate-component-spark[10699]: <13>Mar 20 19:55:55 post-hdfs-component-spark[10724]: + local -r cmd
<13>Mar 20 19:55:55 google-dataproc-startup[10685]: <13>Mar 20 19:55:55 post-hdfs-activate-component-spark[10699]: <13>Mar 20 19:55:55 post-hdfs-component-spark[10724]: + local -r max_retries=30
<13>Mar 20 19:55:55 google-dataproc-startup[10685]: <13>Mar 20 19:55:55 post-hdfs-activate-component-spark[10699]: <13>Mar 20 19:55:55 post-hdfs-component-spark[10724]: + set +x
<13>Mar 20 19:55:55 google-dataproc-startup[10685]: <13>Mar 20 19:55:55 post-hdfs-activate-component-spark[10699]: <13>Mar 20 19:55:55 post-hdfs-component-spark[10724]: About to run 'systemctl enable spark-history-server.service' with retries...
<13>Mar 20 19:55:55 google-dataproc-startup[10685]: <13>Mar 20 19:55:55 post-hdfs-activate-component-spark[10699]: <13>Mar 20 19:55:55 post-hdfs-component-spark[10724]: spark-history-server.service is not a native service, redirecting to systemd-sysv-install.
<13>Mar 20 19:55:55 google-dataproc-startup[10685]: <13>Mar 20 19:55:55 post-hdfs-activate-component-spark[10699]: <13>Mar 20 19:55:55 post-hdfs-component-spark[10724]: Executing: /lib/systemd/systemd-sysv-install enable spark-history-server
<13>Mar 20 19:55:55 google-dataproc-startup[10685]: <13>Mar 20 19:55:55 post-hdfs-activate-component-hive-server2[10695]: + set -euxo pipefail
<13>Mar 20 19:55:55 google-dataproc-startup[10685]: <13>Mar 20 19:55:55 post-hdfs-activate-component-hive-server2[10695]: ++ dirname /usr/local/share/google/dataproc/bdutil/components/post-hdfs/hive-server2.sh
<13>Mar 20 19:55:55 google-dataproc-startup[10685]: <13>Mar 20 19:55:55 post-hdfs-activate-component-hive-server2[10695]: + source /usr/local/share/google/dataproc/bdutil/components/post-hdfs/../../bdutil_env.sh
<13>Mar 20 19:55:55 google-dataproc-startup[10685]: <13>Mar 20 19:55:55 post-hdfs-activate-component-hive-server2[10695]: ++ [[ /usr/local/sbin:/usr/local/bin:/usr/sbin:/usr/bin:/sbin:/bin == *:/snap/bin* ]]
<13>Mar 20 19:55:55 google-dataproc-startup[10685]: <13>Mar 20 19:55:55 post-hdfs-activate-component-hive-server2[10695]: ++ PATH=/usr/local/sbin:/usr/local/bin:/usr/sbin:/usr/bin:/sbin:/bin
<13>Mar 20 19:55:55 google-dataproc-startup[10685]: <13>Mar 20 19:55:55 post-hdfs-activate-component-hive-server2[10695]: ++ DATAPROC_DIR=/usr/local/share/google/dataproc
<13>Mar 20 19:55:55 google-dataproc-startup[10685]: <13>Mar 20 19:55:55 post-hdfs-activate-component-hive-server2[10695]: ++ DATAPROC_TMP_DIR=/tmp/dataproc
<13>Mar 20 19:55:55 google-dataproc-startup[10685]: <13>Mar 20 19:55:55 post-hdfs-activate-component-hive-server2[10695]: ++ DATAPROC_COMMON_LIB_DIR=/usr/local/share/google/dataproc/lib
<13>Mar 20 19:55:55 google-dataproc-startup[10685]: <13>Mar 20 19:55:55 post-hdfs-activate-component-hive-server2[10695]: ++ DATAPROC_ARTIFACTS_DIR=/usr/local/share/google/dataproc/artifacts
<13>Mar 20 19:55:55 google-dataproc-startup[10685]: <13>Mar 20 19:55:55 post-hdfs-activate-component-hive-server2[10695]: ++ DATAPROC_CONF_DIR=/usr/local/share/google/dataproc/conf
<13>Mar 20 19:55:55 google-dataproc-startup[10685]: <13>Mar 20 19:55:55 post-hdfs-activate-component-hive-server2[10695]: ++ DATAPROC_METADATA_CONF=/usr/local/share/google/dataproc/conf/dataproc_metadata.sh
<13>Mar 20 19:55:55 google-dataproc-startup[10685]: <13>Mar 20 19:55:55 post-hdfs-activate-component-hive-server2[10695]: ++ COMMANDS_TMP_DIR=/tmp/dataproc/commands
<13>Mar 20 19:55:55 google-dataproc-startup[10685]: <13>Mar 20 19:55:55 post-hdfs-activate-component-hive-server2[10695]: ++ DATAPROC_COMPONENTS_TMP_DIR=/tmp/dataproc/components
<13>Mar 20 19:55:55 google-dataproc-startup[10685]: <13>Mar 20 19:55:55 post-hdfs-activate-component-hive-server2[10695]: ++ COMPONENTS_TO_ACTIVATE_ENV=/usr/local/share/google/dataproc/bdutil/components/components_to_activate_env.sh
<13>Mar 20 19:55:55 google-dataproc-startup[10685]: <13>Mar 20 19:55:55 post-hdfs-activate-component-hive-server2[10695]: ++ [[ -f /usr/local/share/google/dataproc/conf/dataproc_metadata.sh ]]
<13>Mar 20 19:55:55 google-dataproc-startup[10685]: <13>Mar 20 19:55:55 post-hdfs-activate-component-hive-server2[10695]: ++ INSTALL_GCS_CONNECTOR=1
<13>Mar 20 19:55:55 google-dataproc-startup[10685]: <13>Mar 20 19:55:55 post-hdfs-activate-component-hive-server2[10695]: ++ INSTALL_BIGQUERY_CONNECTOR=1
<13>Mar 20 19:55:55 google-dataproc-startup[10685]: <13>Mar 20 19:55:55 post-hdfs-activate-component-hive-server2[10695]: ++ ENABLE_HDFS=1
<13>Mar 20 19:55:55 google-dataproc-startup[10685]: <13>Mar 20 19:55:55 post-hdfs-activate-component-hive-server2[10695]: ++ ENABLE_HDFS_PERMISSIONS=false
<13>Mar 20 19:55:55 google-dataproc-startup[10685]: <13>Mar 20 19:55:55 post-hdfs-activate-component-hive-server2[10695]: ++ HADOOP_INSTALL_DIR=/usr/lib/hadoop
<13>Mar 20 19:55:55 google-dataproc-startup[10685]: <13>Mar 20 19:55:55 post-hdfs-activate-component-hive-server2[10695]: ++ HADOOP_LIB_DIR=/usr/lib/hadoop/lib
<13>Mar 20 19:55:55 google-dataproc-startup[10685]: <13>Mar 20 19:55:55 post-hdfs-activate-component-hive-server2[10695]: ++ HADOOP_CONF_DIR=/etc/hadoop/conf
<13>Mar 20 19:55:55 google-dataproc-startup[10685]: <13>Mar 20 19:55:55 post-hdfs-activate-component-hive-server2[10695]: ++ HADOOP_MASTER_MAPREDUCE_MEMORY_FRACTION=0.4
<13>Mar 20 19:55:55 google-dataproc-startup[10685]: <13>Mar 20 19:55:55 post-hdfs-activate-component-hive-server2[10695]: ++ HDFS_MASTER_MEMORY_FRACTION=0.4
<13>Mar 20 19:55:55 google-dataproc-startup[10685]: <13>Mar 20 19:55:55 post-hdfs-activate-component-hive-server2[10695]: ++ NODEMANAGER_MEMORY_FRACTION=0.8
<13>Mar 20 19:55:55 google-dataproc-startup[10685]: <13>Mar 20 19:55:55 post-hdfs-activate-component-hive-server2[10695]: ++ NUM_WORKERS=10
<13>Mar 20 19:55:55 google-dataproc-startup[10685]: <13>Mar 20 19:55:55 post-hdfs-activate-component-hive-server2[10695]: ++ WORKERS=()
<13>Mar 20 19:55:55 google-dataproc-startup[10685]: <13>Mar 20 19:55:55 post-hdfs-activate-component-hive-server2[10695]: ++ CORES_PER_MAP_TASK=1.0
<13>Mar 20 19:55:55 google-dataproc-startup[10685]: <13>Mar 20 19:55:55 post-hdfs-activate-component-hive-server2[10695]: ++ CORES_PER_REDUCE_TASK=2.0
<13>Mar 20 19:55:55 google-dataproc-startup[10685]: <13>Mar 20 19:55:55 post-hdfs-activate-component-hive-server2[10695]: ++ CORES_PER_APP_MASTER=2.0
<13>Mar 20 19:55:55 google-dataproc-startup[10685]: <13>Mar 20 19:55:55 post-hdfs-activate-component-hive-server2[10695]: ++ HDFS_DATA_DIRS_PERM=700
<13>Mar 20 19:55:55 google-dataproc-startup[10685]: <13>Mar 20 19:55:55 post-hdfs-activate-component-hive-server2[10695]: ++ [[ -d /usr/lib/jvm/adoptopenjdk-8-hotspot ]]
<13>Mar 20 19:55:55 google-dataproc-startup[10685]: <13>Mar 20 19:55:55 post-hdfs-activate-component-hive-server2[10695]: ++ [[ -d /usr/lib/jvm/adoptopenjdk-8-hotspot-amd64 ]]
<13>Mar 20 19:55:55 google-dataproc-startup[10685]: <13>Mar 20 19:55:55 post-hdfs-activate-component-hive-server2[10695]: ++ JAVA_HOME=/usr/lib/jvm/adoptopenjdk-8-hotspot-amd64
<13>Mar 20 19:55:55 google-dataproc-startup[10685]: <13>Mar 20 19:55:55 post-hdfs-activate-component-hive-server2[10695]: ++ SPARK_CONF_DIR=/etc/spark/conf
<13>Mar 20 19:55:55 google-dataproc-startup[10685]: <13>Mar 20 19:55:55 post-hdfs-activate-component-hive-server2[10695]: ++ SPARK_WORKER_MEMORY_FRACTION=0.8
<13>Mar 20 19:55:55 google-dataproc-startup[10685]: <13>Mar 20 19:55:55 post-hdfs-activate-component-hive-server2[10695]: ++ SPARK_EXECUTOR_MEMORY_FRACTION=0.8
<13>Mar 20 19:55:55 google-dataproc-startup[10685]: <13>Mar 20 19:55:55 post-hdfs-activate-component-hive-server2[10695]: ++ SPARK_DAEMON_MEMORY_FRACTION=0.15
<13>Mar 20 19:55:55 google-dataproc-startup[10685]: <13>Mar 20 19:55:55 post-hdfs-activate-component-hive-server2[10695]: ++ SPARK_EXECUTORS_PER_VM=2
<13>Mar 20 19:55:55 google-dataproc-startup[10685]: <13>Mar 20 19:55:55 post-hdfs-activate-component-hive-server2[10695]: ++ TEZ_CONF_DIR=/etc/tez/conf
<13>Mar 20 19:55:55 google-dataproc-startup[10685]: <13>Mar 20 19:55:55 post-hdfs-activate-component-hive-server2[10695]: ++ TEZ_LIB_DIR=/usr/lib/tez
<13>Mar 20 19:55:55 google-dataproc-startup[10685]: <13>Mar 20 19:55:55 post-hdfs-activate-component-yarn[10700]: + local -r component=yarn
<13>Mar 20 19:55:55 google-dataproc-startup[10685]: <13>Mar 20 19:55:55 post-hdfs-activate-component-yarn[10700]: + local -r post_hdfs_script=/usr/local/share/google/dataproc/bdutil/components/post-hdfs/yarn.sh
<13>Mar 20 19:55:55 google-dataproc-startup[10685]: <13>Mar 20 19:55:55 post-hdfs-activate-component-yarn[10700]: + [[ -f /usr/local/share/google/dataproc/bdutil/components/post-hdfs/yarn.sh ]]
<13>Mar 20 19:55:55 google-dataproc-startup[10685]: <13>Mar 20 19:55:55 post-hdfs-activate-component-yarn[10700]: + echo 'Component yarn doesn'\''t have a post-hdfs script'
<13>Mar 20 19:55:55 google-dataproc-startup[10685]: <13>Mar 20 19:55:55 post-hdfs-activate-component-yarn[10700]: Component yarn doesn't have a post-hdfs script
<13>Mar 20 19:55:55 google-dataproc-startup[10685]: <13>Mar 20 19:55:55 post-hdfs-activate-component-mysql[10698]: + local -r component=mysql
<13>Mar 20 19:55:55 google-dataproc-startup[10685]: <13>Mar 20 19:55:55 post-hdfs-activate-component-mysql[10698]: + local -r post_hdfs_script=/usr/local/share/google/dataproc/bdutil/components/post-hdfs/mysql.sh
<13>Mar 20 19:55:55 google-dataproc-startup[10685]: <13>Mar 20 19:55:55 post-hdfs-activate-component-mysql[10698]: + [[ -f /usr/local/share/google/dataproc/bdutil/components/post-hdfs/mysql.sh ]]
<13>Mar 20 19:55:55 google-dataproc-startup[10685]: <13>Mar 20 19:55:55 post-hdfs-activate-component-mysql[10698]: + echo 'Component mysql doesn'\''t have a post-hdfs script'
<13>Mar 20 19:55:55 google-dataproc-startup[10685]: <13>Mar 20 19:55:55 post-hdfs-activate-component-mysql[10698]: Component mysql doesn't have a post-hdfs script
<13>Mar 20 19:55:55 google-dataproc-startup[10685]: <13>Mar 20 19:55:55 post-hdfs-activate-component-hive-server2[10695]: ++ dirname /usr/local/share/google/dataproc/bdutil/components/post-hdfs/hive-server2.sh
<13>Mar 20 19:55:55 google-dataproc-startup[10685]: <13>Mar 20 19:55:55 post-hdfs-activate-component-hive-server2[10695]: + source /usr/local/share/google/dataproc/bdutil/components/post-hdfs/../../bdutil_helpers.sh
<13>Mar 20 19:55:55 google-dataproc-startup[10685]: <13>Mar 20 19:55:55 post-hdfs-activate-component-hive-server2[10695]: ++ is_centos
<13>Mar 20 19:55:55 google-dataproc-startup[10685]: <13>Mar 20 19:55:55 post-hdfs-activate-component-mapreduce[10696]: + local -r component=mapreduce
<13>Mar 20 19:55:55 google-dataproc-startup[10685]: <13>Mar 20 19:55:55 post-hdfs-activate-component-mapreduce[10696]: + local -r post_hdfs_script=/usr/local/share/google/dataproc/bdutil/components/post-hdfs/mapreduce.sh
<13>Mar 20 19:55:55 google-dataproc-startup[10685]: <13>Mar 20 19:55:55 post-hdfs-activate-component-mapreduce[10696]: + [[ -f /usr/local/share/google/dataproc/bdutil/components/post-hdfs/mapreduce.sh ]]
<13>Mar 20 19:55:55 google-dataproc-startup[10685]: <13>Mar 20 19:55:55 post-hdfs-activate-component-mapreduce[10696]: + echo 'Component mapreduce doesn'\''t have a post-hdfs script'
<13>Mar 20 19:55:55 google-dataproc-startup[10685]: <13>Mar 20 19:55:55 post-hdfs-activate-component-mapreduce[10696]: Component mapreduce doesn't have a post-hdfs script
<13>Mar 20 19:55:55 google-dataproc-startup[10685]: <13>Mar 20 19:55:55 post-hdfs-activate-component-hive-server2[10695]: +++ . /etc/os-release
<13>Mar 20 19:55:55 google-dataproc-startup[10685]: <13>Mar 20 19:55:55 post-hdfs-activate-component-hive-server2[10695]: ++++ PRETTY_NAME='Debian GNU/Linux 10 (buster)'
<13>Mar 20 19:55:55 google-dataproc-startup[10685]: <13>Mar 20 19:55:55 post-hdfs-activate-component-hive-server2[10695]: ++++ NAME='Debian GNU/Linux'
<13>Mar 20 19:55:55 google-dataproc-startup[10685]: <13>Mar 20 19:55:55 post-hdfs-activate-component-hive-server2[10695]: ++++ VERSION_ID=10
<13>Mar 20 19:55:55 google-dataproc-startup[10685]: <13>Mar 20 19:55:55 post-hdfs-activate-component-hive-server2[10695]: ++++ VERSION='10 (buster)'
<13>Mar 20 19:55:55 google-dataproc-startup[10685]: <13>Mar 20 19:55:55 post-hdfs-activate-component-hive-server2[10695]: ++++ VERSION_CODENAME=buster
<13>Mar 20 19:55:55 google-dataproc-startup[10685]: <13>Mar 20 19:55:55 post-hdfs-activate-component-hive-server2[10695]: ++++ ID=debian
<13>Mar 20 19:55:55 google-dataproc-startup[10685]: <13>Mar 20 19:55:55 post-hdfs-activate-component-hive-server2[10695]: ++++ HOME_URL=https://www.debian.org/
<13>Mar 20 19:55:55 google-dataproc-startup[10685]: <13>Mar 20 19:55:55 post-hdfs-activate-component-hive-server2[10695]: ++++ SUPPORT_URL=https://www.debian.org/support
<13>Mar 20 19:55:55 google-dataproc-startup[10685]: <13>Mar 20 19:55:55 post-hdfs-activate-component-hive-server2[10695]: ++++ BUG_REPORT_URL=https://bugs.debian.org/
<13>Mar 20 19:55:55 google-dataproc-startup[10685]: <13>Mar 20 19:55:55 post-hdfs-activate-component-hive-server2[10695]: +++ echo debian
<13>Mar 20 19:55:55 google-dataproc-startup[10685]: <13>Mar 20 19:55:55 post-hdfs-activate-component-hive-server2[10695]: ++ [[ debian == \c\e\n\t\o\s ]]
<13>Mar 20 19:55:55 google-dataproc-startup[10685]: <13>Mar 20 19:55:55 post-hdfs-activate-component-hive-server2[10695]: ++ return 1
<13>Mar 20 19:55:55 google-dataproc-startup[10685]: <13>Mar 20 19:55:55 post-hdfs-activate-component-hive-server2[10695]: ++ source /usr/local/share/google/dataproc/bdutil/os/debian/bdutil_helpers.sh
<13>Mar 20 19:55:55 google-dataproc-startup[10685]: <13>Mar 20 19:55:55 post-hdfs-activate-component-hive-server2[10695]: +++ APT_SENTINEL=apt.lastupdate
<13>Mar 20 19:55:55 google-dataproc-startup[10685]: <13>Mar 20 19:55:55 post-hdfs-activate-component-hive-server2[10695]: ++ readonly EXIT_CODE_INTERNAL_ERROR=1
<13>Mar 20 19:55:55 google-dataproc-startup[10685]: <13>Mar 20 19:55:55 post-hdfs-activate-component-hive-server2[10695]: ++ EXIT_CODE_INTERNAL_ERROR=1
<13>Mar 20 19:55:55 google-dataproc-startup[10685]: <13>Mar 20 19:55:55 post-hdfs-activate-component-hive-server2[10695]: ++ readonly EXIT_CODE_CLIENT_ERROR=2
<13>Mar 20 19:55:55 google-dataproc-startup[10685]: <13>Mar 20 19:55:55 post-hdfs-activate-component-hive-server2[10695]: ++ EXIT_CODE_CLIENT_ERROR=2
<13>Mar 20 19:55:55 google-dataproc-startup[10685]: <13>Mar 20 19:55:55 post-hdfs-activate-component-hive-server2[10695]: ++ dirname /usr/local/share/google/dataproc/bdutil/components/post-hdfs/hive-server2.sh
<13>Mar 20 19:55:55 google-dataproc-startup[10685]: <13>Mar 20 19:55:55 post-hdfs-activate-component-hive-server2[10695]: + source /usr/local/share/google/dataproc/bdutil/components/post-hdfs/../../cluster_properties.sh
<13>Mar 20 19:55:55 google-dataproc-startup[10685]: <13>Mar 20 19:55:55 post-hdfs-activate-component-hive-server2[10695]: ++ dirname /usr/local/share/google/dataproc/bdutil/components/post-hdfs/hive-server2.sh
<13>Mar 20 19:55:55 google-dataproc-startup[10685]: <13>Mar 20 19:55:55 post-hdfs-activate-component-hive-server2[10695]: + source /usr/local/share/google/dataproc/bdutil/components/post-hdfs/../shared/hive.sh
<13>Mar 20 19:55:55 google-dataproc-startup[10685]: <13>Mar 20 19:55:55 post-hdfs-activate-component-hive-server2[10695]: ++ HIVE_CONF_DIR=/etc/hive/conf
<13>Mar 20 19:55:55 google-dataproc-startup[10685]: <13>Mar 20 19:55:55 post-hdfs-activate-component-hive-server2[10695]: + [[ true == \t\r\u\e ]]
<13>Mar 20 19:55:55 google-dataproc-startup[10685]: <13>Mar 20 19:55:55 post-hdfs-activate-component-hive-server2[10695]: + start_hive_server2
<13>Mar 20 19:55:55 google-dataproc-startup[10685]: <13>Mar 20 19:55:55 post-hdfs-activate-component-hive-server2[10695]: + wait_for_hive_metastore
<13>Mar 20 19:55:55 google-dataproc-startup[10685]: <13>Mar 20 19:55:55 post-hdfs-activate-component-hive-server2[10695]: + local timeout
<13>Mar 20 19:55:55 google-dataproc-startup[10685]: <13>Mar 20 19:55:55 post-hdfs-activate-component-hive-server2[10695]: ++ get_dataproc_property_or_default startup.component.service-binding-timeout.hive-metastore 300
<13>Mar 20 19:55:55 google-dataproc-startup[10685]: <13>Mar 20 19:55:55 post-hdfs-activate-component-hive-server2[10695]: ++ local -r property_name=startup.component.service-binding-timeout.hive-metastore
<13>Mar 20 19:55:55 google-dataproc-startup[10685]: <13>Mar 20 19:55:55 post-hdfs-activate-component-hive-server2[10695]: ++ local -r default_value=300
<13>Mar 20 19:55:55 google-dataproc-startup[10685]: <13>Mar 20 19:55:55 post-hdfs-activate-component-hive-server2[10695]: ++ local actual_value
<13>Mar 20 19:55:55 google-dataproc-startup[10685]: <13>Mar 20 19:55:55 post-hdfs-activate-component-hive-server2[10695]: +++ get_dataproc_property startup.component.service-binding-timeout.hive-metastore
<13>Mar 20 19:55:55 google-dataproc-startup[10685]: <13>Mar 20 19:55:55 post-hdfs-activate-component-hive-server2[10695]: +++ local -r property_name=startup.component.service-binding-timeout.hive-metastore
<13>Mar 20 19:55:55 google-dataproc-startup[10685]: <13>Mar 20 19:55:55 post-hdfs-activate-component-hive-server2[10695]: +++ local property_value
<13>Mar 20 19:55:55 google-dataproc-startup[10685]: <13>Mar 20 19:55:55 post-hdfs-activate-component-hive-server2[10695]: ++++ get_java_property /etc/google-dataproc/dataproc.properties startup.component.service-binding-timeout.hive-metastore
<13>Mar 20 19:55:55 google-dataproc-startup[10685]: <13>Mar 20 19:55:55 post-hdfs-activate-component-hive-server2[10695]: ++++ local -r property_file=/etc/google-dataproc/dataproc.properties
<13>Mar 20 19:55:55 google-dataproc-startup[10685]: <13>Mar 20 19:55:55 post-hdfs-activate-component-hive-server2[10695]: ++++ local -r property_name=startup.component.service-binding-timeout.hive-metastore
<13>Mar 20 19:55:55 google-dataproc-startup[10685]: <13>Mar 20 19:55:55 post-hdfs-activate-component-hive-server2[10695]: ++++ local property_value
<13>Mar 20 19:55:55 google-dataproc-startup[10685]: <13>Mar 20 19:55:55 post-hdfs-activate-component-hive-server2[10695]: +++++ tail -n 1
<13>Mar 20 19:55:55 google-dataproc-startup[10685]: <13>Mar 20 19:55:55 post-hdfs-activate-component-hive-server2[10695]: +++++ grep '^startup.component.service-binding-timeout.hive-metastore=' /etc/google-dataproc/dataproc.properties
<13>Mar 20 19:55:55 google-dataproc-startup[10685]: <13>Mar 20 19:55:55 post-hdfs-activate-component-hive-server2[10695]: +++++ sed -r 's/\\([#!=:])/\1/g'
<13>Mar 20 19:55:55 google-dataproc-startup[10685]: <13>Mar 20 19:55:55 post-hdfs-activate-component-hive-server2[10695]: +++++ cut -d = -f 2-
<13>Mar 20 19:55:55 google-dataproc-startup[10685]: <13>Mar 20 19:55:55 post-hdfs-activate-component-hive-server2[10695]: ++++ property_value=
<13>Mar 20 19:55:55 google-dataproc-startup[10685]: <13>Mar 20 19:55:55 post-hdfs-activate-component-hive-server2[10695]: ++++ echo ''
<13>Mar 20 19:55:55 google-dataproc-startup[10685]: <13>Mar 20 19:55:55 post-hdfs-activate-component-hive-server2[10695]: +++ property_value=
<13>Mar 20 19:55:55 google-dataproc-startup[10685]: <13>Mar 20 19:55:55 post-hdfs-activate-component-hive-server2[10695]: +++ echo ''
<13>Mar 20 19:55:55 google-dataproc-startup[10685]: <13>Mar 20 19:55:55 post-hdfs-activate-component-hive-server2[10695]: ++ actual_value=
<13>Mar 20 19:55:55 google-dataproc-startup[10685]: <13>Mar 20 19:55:55 post-hdfs-activate-component-hive-server2[10695]: ++ [[ -n '' ]]
<13>Mar 20 19:55:55 google-dataproc-startup[10685]: <13>Mar 20 19:55:55 post-hdfs-activate-component-hive-server2[10695]: ++ echo 300
<13>Mar 20 19:55:55 google-dataproc-startup[10685]: <13>Mar 20 19:55:55 post-hdfs-activate-component-hive-server2[10695]: + timeout=300
<13>Mar 20 19:55:55 google-dataproc-startup[10685]: <13>Mar 20 19:55:55 post-hdfs-activate-component-hive-server2[10695]: + local metastore_uris
<13>Mar 20 19:55:55 google-dataproc-startup[10685]: <13>Mar 20 19:55:55 post-hdfs-activate-component-hive-server2[10695]: ++ get_property_in_xml /etc/hive/conf/hive-site.xml hive.metastore.uris
<13>Mar 20 19:55:55 google-dataproc-startup[10685]: <13>Mar 20 19:55:55 post-hdfs-activate-component-hive-server2[10695]: ++ local -r path=/etc/hive/conf/hive-site.xml
<13>Mar 20 19:55:55 google-dataproc-startup[10685]: <13>Mar 20 19:55:55 post-hdfs-activate-component-hive-server2[10695]: ++ local -r property=hive.metastore.uris
<13>Mar 20 19:55:55 google-dataproc-startup[10685]: <13>Mar 20 19:55:55 post-hdfs-activate-component-hive-server2[10695]: ++ local -r default_value=
<13>Mar 20 19:55:55 google-dataproc-startup[10685]: <13>Mar 20 19:55:55 post-hdfs-activate-component-hive-server2[10695]: ++ local val
<13>Mar 20 19:55:55 google-dataproc-startup[10685]: <13>Mar 20 19:55:55 post-hdfs-activate-component-hive-server2[10695]: +++ bdconfig get_property_value --configuration_file /etc/hive/conf/hive-site.xml --name hive.metastore.uris
<13>Mar 20 19:55:55 google-dataproc-startup[10685]: <13>Mar 20 19:55:55 post-hdfs-activate-component-hive-server2[10695]: ++ val=thrift://tianzhenyv-qwiklab-m:9083
<13>Mar 20 19:55:55 google-dataproc-startup[10685]: <13>Mar 20 19:55:55 post-hdfs-activate-component-hive-server2[10695]: ++ [[ thrift://tianzhenyv-qwiklab-m:9083 == \N\o\n\e ]]
<13>Mar 20 19:55:55 google-dataproc-startup[10685]: <13>Mar 20 19:55:55 post-hdfs-activate-component-hive-server2[10695]: ++ echo thrift://tianzhenyv-qwiklab-m:9083
<13>Mar 20 19:55:55 google-dataproc-startup[10685]: <13>Mar 20 19:55:55 post-hdfs-activate-component-hive-server2[10695]: + metastore_uris=thrift://tianzhenyv-qwiklab-m:9083
<13>Mar 20 19:55:55 google-dataproc-startup[10685]: <13>Mar 20 19:55:55 post-hdfs-activate-component-hive-server2[10695]: + local host
<13>Mar 20 19:55:55 google-dataproc-startup[10685]: <13>Mar 20 19:55:55 post-hdfs-activate-component-hive-server2[10695]: ++ echo thrift://tianzhenyv-qwiklab-m:9083
<13>Mar 20 19:55:55 google-dataproc-startup[10685]: <13>Mar 20 19:55:55 post-hdfs-activate-component-hive-server2[10695]: ++ sed -n 's#.*://\(.*\):.*#\1#p'
<13>Mar 20 19:55:55 google-dataproc-startup[10685]: <13>Mar 20 19:55:55 post-hdfs-activate-component-hive-server2[10695]: + host=tianzhenyv-qwiklab-m
<13>Mar 20 19:55:55 google-dataproc-startup[10685]: <13>Mar 20 19:55:55 post-hdfs-activate-component-hive-server2[10695]: + [[ -z tianzhenyv-qwiklab-m ]]
<13>Mar 20 19:55:55 google-dataproc-startup[10685]: <13>Mar 20 19:55:55 post-hdfs-activate-component-hive-server2[10695]: + local port
<13>Mar 20 19:55:55 google-dataproc-startup[10685]: <13>Mar 20 19:55:55 post-hdfs-activate-component-hive-server2[10695]: ++ echo thrift://tianzhenyv-qwiklab-m:9083
<13>Mar 20 19:55:55 google-dataproc-startup[10685]: <13>Mar 20 19:55:55 post-hdfs-activate-component-hive-server2[10695]: ++ sed -n 's#.*://.*:\(.*\)#\1#p'
<13>Mar 20 19:55:55 google-dataproc-startup[10685]: <13>Mar 20 19:55:55 post-hdfs-activate-component-hive-server2[10695]: + port=9083
<13>Mar 20 19:55:55 google-dataproc-startup[10685]: <13>Mar 20 19:55:55 post-hdfs-activate-component-hive-server2[10695]: + [[ -z 9083 ]]
<13>Mar 20 19:55:55 google-dataproc-startup[10685]: <13>Mar 20 19:55:55 post-hdfs-activate-component-hive-server2[10695]: + wait_for_port hive-metastore tianzhenyv-qwiklab-m 9083 300
<13>Mar 20 19:55:55 google-dataproc-startup[10685]: <13>Mar 20 19:55:55 post-hdfs-activate-component-hive-server2[10695]: + local -r name=hive-metastore
<13>Mar 20 19:55:55 google-dataproc-startup[10685]: <13>Mar 20 19:55:55 post-hdfs-activate-component-hive-server2[10695]: + local -r host=tianzhenyv-qwiklab-m
<13>Mar 20 19:55:55 google-dataproc-startup[10685]: <13>Mar 20 19:55:55 post-hdfs-activate-component-hive-server2[10695]: + local -r port=9083
<13>Mar 20 19:55:55 google-dataproc-startup[10685]: <13>Mar 20 19:55:55 post-hdfs-activate-component-hive-server2[10695]: + local -r timeout=300
<13>Mar 20 19:55:55 google-dataproc-startup[10685]: <13>Mar 20 19:55:55 post-hdfs-activate-component-hive-server2[10695]: + local -r capped_timeout=300
<13>Mar 20 19:55:55 google-dataproc-startup[10685]: <13>Mar 20 19:55:55 post-hdfs-activate-component-hive-server2[10695]: + loginfo 'Waiting 300 seconds for service to come up on host=tianzhenyv-qwiklab-m port=9083 name=hive-metastore.'
<13>Mar 20 19:55:55 google-dataproc-startup[10685]: <13>Mar 20 19:55:55 post-hdfs-activate-component-hive-server2[10695]: + echo 'Waiting 300 seconds for service to come up on host=tianzhenyv-qwiklab-m port=9083 name=hive-metastore.'
<13>Mar 20 19:55:55 google-dataproc-startup[10685]: <13>Mar 20 19:55:55 post-hdfs-activate-component-hive-server2[10695]: Waiting 300 seconds for service to come up on host=tianzhenyv-qwiklab-m port=9083 name=hive-metastore.
<13>Mar 20 19:55:55 google-dataproc-startup[10685]: <13>Mar 20 19:55:55 post-hdfs-activate-component-hive-server2[10695]: + retry_constant_custom 300 1 nc -v -z -w 1 tianzhenyv-qwiklab-m 9083
<13>Mar 20 19:55:55 google-dataproc-startup[10685]: <13>Mar 20 19:55:55 post-hdfs-activate-component-hive-server2[10695]: + local -r max_retry_time=300
<13>Mar 20 19:55:55 google-dataproc-startup[10685]: <13>Mar 20 19:55:55 post-hdfs-activate-component-hive-server2[10695]: + local -r retry_delay=1
<13>Mar 20 19:55:55 google-dataproc-startup[10685]: <13>Mar 20 19:55:55 post-hdfs-activate-component-hive-server2[10695]: + cmd=("${@:3}")
<13>Mar 20 19:55:55 google-dataproc-startup[10685]: <13>Mar 20 19:55:55 post-hdfs-activate-component-hive-server2[10695]: + local -r cmd
<13>Mar 20 19:55:55 google-dataproc-startup[10685]: <13>Mar 20 19:55:55 post-hdfs-activate-component-hive-server2[10695]: + local -r max_retries=300
<13>Mar 20 19:55:55 google-dataproc-startup[10685]: <13>Mar 20 19:55:55 post-hdfs-activate-component-hive-server2[10695]: + set +x
<13>Mar 20 19:55:55 google-dataproc-startup[10685]: <13>Mar 20 19:55:55 post-hdfs-activate-component-hive-server2[10695]: About to run 'nc -v -z -w 1 tianzhenyv-qwiklab-m 9083' with retries...
<13>Mar 20 19:55:55 google-dataproc-startup[10685]: <13>Mar 20 19:55:55 post-hdfs-activate-component-hive-server2[10695]: Connection to tianzhenyv-qwiklab-m 9083 port [tcp/*] succeeded!
<13>Mar 20 19:55:55 google-dataproc-startup[10685]: <13>Mar 20 19:55:55 post-hdfs-activate-component-hive-server2[10695]: nc -v -z -w 1 tianzhenyv-qwiklab-m 9083 succeeded.
<13>Mar 20 19:55:55 google-dataproc-startup[10685]: <13>Mar 20 19:55:55 post-hdfs-activate-component-hive-server2[10695]: + return 0
<13>Mar 20 19:55:55 google-dataproc-startup[10685]: <13>Mar 20 19:55:55 post-hdfs-activate-component-hive-server2[10695]: + loginfo 'Service up on host=tianzhenyv-qwiklab-m port=9083 name=hive-metastore.'
<13>Mar 20 19:55:55 google-dataproc-startup[10685]: <13>Mar 20 19:55:55 post-hdfs-activate-component-hive-server2[10695]: + echo 'Service up on host=tianzhenyv-qwiklab-m port=9083 name=hive-metastore.'
<13>Mar 20 19:55:55 google-dataproc-startup[10685]: <13>Mar 20 19:55:55 post-hdfs-activate-component-hive-server2[10695]: Service up on host=tianzhenyv-qwiklab-m port=9083 name=hive-metastore.
<13>Mar 20 19:55:55 google-dataproc-startup[10685]: <13>Mar 20 19:55:55 post-hdfs-activate-component-hive-server2[10695]: + enable_service hive-server2
<13>Mar 20 19:55:55 google-dataproc-startup[10685]: <13>Mar 20 19:55:55 post-hdfs-activate-component-hive-server2[10695]: + local -r service=hive-server2
<13>Mar 20 19:55:55 google-dataproc-startup[10685]: <13>Mar 20 19:55:55 post-hdfs-activate-component-hive-server2[10695]: + local -r unit=hive-server2.service
<13>Mar 20 19:55:55 google-dataproc-startup[10685]: <13>Mar 20 19:55:55 post-hdfs-activate-component-hive-server2[10695]: + retry_constant_short systemctl enable hive-server2.service
<13>Mar 20 19:55:55 google-dataproc-startup[10685]: <13>Mar 20 19:55:55 post-hdfs-activate-component-hive-server2[10695]: + retry_constant_custom 30 1 systemctl enable hive-server2.service
<13>Mar 20 19:55:55 google-dataproc-startup[10685]: <13>Mar 20 19:55:55 post-hdfs-activate-component-hive-server2[10695]: + local -r max_retry_time=30
<13>Mar 20 19:55:55 google-dataproc-startup[10685]: <13>Mar 20 19:55:55 post-hdfs-activate-component-hive-server2[10695]: + local -r retry_delay=1
<13>Mar 20 19:55:55 google-dataproc-startup[10685]: <13>Mar 20 19:55:55 post-hdfs-activate-component-hive-server2[10695]: + cmd=("${@:3}")
<13>Mar 20 19:55:55 google-dataproc-startup[10685]: <13>Mar 20 19:55:55 post-hdfs-activate-component-hive-server2[10695]: + local -r cmd
<13>Mar 20 19:55:55 google-dataproc-startup[10685]: <13>Mar 20 19:55:55 post-hdfs-activate-component-hive-server2[10695]: + local -r max_retries=30
<13>Mar 20 19:55:55 google-dataproc-startup[10685]: <13>Mar 20 19:55:55 post-hdfs-activate-component-hive-server2[10695]: + set +x
<13>Mar 20 19:55:55 google-dataproc-startup[10685]: <13>Mar 20 19:55:55 post-hdfs-activate-component-hive-server2[10695]: About to run 'systemctl enable hive-server2.service' with retries...
<13>Mar 20 19:55:55 google-dataproc-startup[10685]: <13>Mar 20 19:55:55 post-hdfs-activate-component-hive-server2[10695]: hive-server2.service is not a native service, redirecting to systemd-sysv-install.
<13>Mar 20 19:55:55 google-dataproc-startup[10685]: <13>Mar 20 19:55:55 post-hdfs-activate-component-hive-server2[10695]: Executing: /lib/systemd/systemd-sysv-install enable hive-server2
<13>Mar 20 19:55:56 google-dataproc-startup[10685]: <13>Mar 20 19:55:56 post-hdfs-activate-component-spark[10699]: <13>Mar 20 19:55:56 post-hdfs-component-spark[10724]: systemctl enable spark-history-server.service succeeded.
<13>Mar 20 19:55:56 google-dataproc-startup[10685]: <13>Mar 20 19:55:56 post-hdfs-activate-component-spark[10699]: <13>Mar 20 19:55:56 post-hdfs-component-spark[10724]: + return 0
<13>Mar 20 19:55:56 google-dataproc-startup[10685]: <13>Mar 20 19:55:56 post-hdfs-activate-component-spark[10699]: <13>Mar 20 19:55:56 post-hdfs-component-spark[10724]: + local -r common_restart_drop_in=/etc/systemd/system/common/restart.conf
<13>Mar 20 19:55:56 google-dataproc-startup[10685]: <13>Mar 20 19:55:56 post-hdfs-activate-component-spark[10699]: <13>Mar 20 19:55:56 post-hdfs-component-spark[10724]: + [[ ! -f /etc/systemd/system/common/restart.conf ]]
<13>Mar 20 19:55:56 google-dataproc-startup[10685]: <13>Mar 20 19:55:56 post-hdfs-activate-component-spark[10699]: <13>Mar 20 19:55:56 post-hdfs-component-spark[10724]: + local -r worker_restart_drop_in=/etc/systemd/system/common/worker-restart.conf
<13>Mar 20 19:55:56 google-dataproc-startup[10685]: <13>Mar 20 19:55:56 post-hdfs-activate-component-spark[10699]: <13>Mar 20 19:55:56 post-hdfs-component-spark[10724]: + [[ ! -f /etc/systemd/system/common/worker-restart.conf ]]
<13>Mar 20 19:55:56 google-dataproc-startup[10685]: <13>Mar 20 19:55:56 post-hdfs-activate-component-spark[10699]: <13>Mar 20 19:55:56 post-hdfs-component-spark[10724]: + local -r drop_in_dir=/etc/systemd/system/spark-history-server.service.d
<13>Mar 20 19:55:56 google-dataproc-startup[10685]: <13>Mar 20 19:55:56 post-hdfs-activate-component-spark[10699]: <13>Mar 20 19:55:56 post-hdfs-component-spark[10724]: + mkdir -p /etc/systemd/system/spark-history-server.service.d
<13>Mar 20 19:55:56 google-dataproc-startup[10685]: <13>Mar 20 19:55:56 post-hdfs-activate-component-spark[10699]: <13>Mar 20 19:55:56 post-hdfs-component-spark[10724]: + local props
<13>Mar 20 19:55:56 google-dataproc-startup[10685]: <13>Mar 20 19:55:56 post-hdfs-activate-component-spark[10699]: <13>Mar 20 19:55:56 post-hdfs-component-spark[10724]: ++ systemctl show spark-history-server.service -p Restart,RemainAfterExit
<13>Mar 20 19:55:56 google-dataproc-startup[10685]: <13>Mar 20 19:55:56 post-hdfs-activate-component-spark[10699]: <13>Mar 20 19:55:56 post-hdfs-component-spark[10724]: + props='Restart=no
<13>Mar 20 19:55:56 google-dataproc-startup[10685]: <13>Mar 20 19:55:56 post-hdfs-activate-component-spark[10699]: <13>Mar 20 19:55:56 post-hdfs-component-spark[10724]: RemainAfterExit=no'
<13>Mar 20 19:55:56 google-dataproc-startup[10685]: <13>Mar 20 19:55:56 post-hdfs-activate-component-spark[10699]: <13>Mar 20 19:55:56 post-hdfs-component-spark[10724]: + [[ spark-history-server != \h\a\d\o\o\p\-\h\d\f\s\-\d\a\t\a\n\o\d\e ]]
<13>Mar 20 19:55:56 google-dataproc-startup[10685]: <13>Mar 20 19:55:56 post-hdfs-activate-component-spark[10699]: <13>Mar 20 19:55:56 post-hdfs-component-spark[10724]: + [[ spark-history-server != \h\a\d\o\o\p\-\y\a\r\n\-\n\o\d\e\m\a\n\a\g\e\r ]]
<13>Mar 20 19:55:56 google-dataproc-startup[10685]: <13>Mar 20 19:55:56 post-hdfs-activate-component-spark[10699]: <13>Mar 20 19:55:56 post-hdfs-component-spark[10724]: + [[ Restart=no
<13>Mar 20 19:55:56 google-dataproc-startup[10685]: <13>Mar 20 19:55:56 post-hdfs-activate-component-spark[10699]: <13>Mar 20 19:55:56 post-hdfs-component-spark[10724]: RemainAfterExit=no == *\R\e\s\t\a\r\t\=\n\o* ]]
<13>Mar 20 19:55:56 google-dataproc-startup[10685]: <13>Mar 20 19:55:56 post-hdfs-activate-component-spark[10699]: <13>Mar 20 19:55:56 post-hdfs-component-spark[10724]: + [[ Restart=no
<13>Mar 20 19:55:56 google-dataproc-startup[10685]: <13>Mar 20 19:55:56 post-hdfs-activate-component-spark[10699]: <13>Mar 20 19:55:56 post-hdfs-component-spark[10724]: RemainAfterExit=no == *\R\e\m\a\i\n\A\f\t\e\r\E\x\i\t\=\n\o* ]]
<13>Mar 20 19:55:56 google-dataproc-startup[10685]: <13>Mar 20 19:55:56 post-hdfs-activate-component-spark[10699]: <13>Mar 20 19:55:56 post-hdfs-component-spark[10724]: + ln -s -f /etc/systemd/system/common/restart.conf /etc/systemd/system/spark-history-server.service.d
<13>Mar 20 19:55:56 google-dataproc-startup[10685]: <13>Mar 20 19:55:56 post-hdfs-activate-component-spark[10699]: <13>Mar 20 19:55:56 post-hdfs-component-spark[10724]: + local -r gate_start_on_agent_success_drop_in=/etc/systemd/system/common/agent-gate.conf
<13>Mar 20 19:55:56 google-dataproc-startup[10685]: <13>Mar 20 19:55:56 post-hdfs-activate-component-spark[10699]: <13>Mar 20 19:55:56 post-hdfs-component-spark[10724]: + [[ ! -f /etc/systemd/system/common/agent-gate.conf ]]
<13>Mar 20 19:55:56 google-dataproc-startup[10685]: <13>Mar 20 19:55:56 post-hdfs-activate-component-spark[10699]: <13>Mar 20 19:55:56 post-hdfs-component-spark[10724]: + [[ spark-history-server == \h\a\d\o\o\p\-\h\d\f\s\-\d\a\t\a\n\o\d\e ]]
<13>Mar 20 19:55:56 google-dataproc-startup[10685]: <13>Mar 20 19:55:56 post-hdfs-activate-component-spark[10699]: <13>Mar 20 19:55:56 post-hdfs-component-spark[10724]: + [[ spark-history-server == \h\a\d\o\o\p\-\y\a\r\n\-\n\o\d\e\m\a\n\a\g\e\r ]]
<13>Mar 20 19:55:56 google-dataproc-startup[10685]: <13>Mar 20 19:55:56 post-hdfs-activate-component-spark[10699]: <13>Mar 20 19:55:56 post-hdfs-component-spark[10724]: + retry_constant systemctl start spark-history-server
<13>Mar 20 19:55:56 google-dataproc-startup[10685]: <13>Mar 20 19:55:56 post-hdfs-activate-component-spark[10699]: <13>Mar 20 19:55:56 post-hdfs-component-spark[10724]: + retry_constant_custom 300 1 systemctl start spark-history-server
<13>Mar 20 19:55:56 google-dataproc-startup[10685]: <13>Mar 20 19:55:56 post-hdfs-activate-component-spark[10699]: <13>Mar 20 19:55:56 post-hdfs-component-spark[10724]: + local -r max_retry_time=300
<13>Mar 20 19:55:56 google-dataproc-startup[10685]: <13>Mar 20 19:55:56 post-hdfs-activate-component-spark[10699]: <13>Mar 20 19:55:56 post-hdfs-component-spark[10724]: + local -r retry_delay=1
<13>Mar 20 19:55:56 google-dataproc-startup[10685]: <13>Mar 20 19:55:56 post-hdfs-activate-component-spark[10699]: <13>Mar 20 19:55:56 post-hdfs-component-spark[10724]: + cmd=("${@:3}")
<13>Mar 20 19:55:56 google-dataproc-startup[10685]: <13>Mar 20 19:55:56 post-hdfs-activate-component-spark[10699]: <13>Mar 20 19:55:56 post-hdfs-component-spark[10724]: + local -r cmd
<13>Mar 20 19:55:56 google-dataproc-startup[10685]: <13>Mar 20 19:55:56 post-hdfs-activate-component-spark[10699]: <13>Mar 20 19:55:56 post-hdfs-component-spark[10724]: + local -r max_retries=300
<13>Mar 20 19:55:56 google-dataproc-startup[10685]: <13>Mar 20 19:55:56 post-hdfs-activate-component-spark[10699]: <13>Mar 20 19:55:56 post-hdfs-component-spark[10724]: + set +x
<13>Mar 20 19:55:56 google-dataproc-startup[10685]: <13>Mar 20 19:55:56 post-hdfs-activate-component-spark[10699]: <13>Mar 20 19:55:56 post-hdfs-component-spark[10724]: About to run 'systemctl start spark-history-server' with retries...
<13>Mar 20 19:55:56 google-dataproc-startup[10685]: <13>Mar 20 19:55:56 post-hdfs-activate-component-hive-server2[10695]: systemctl enable hive-server2.service succeeded.
<13>Mar 20 19:55:56 google-dataproc-startup[10685]: <13>Mar 20 19:55:56 post-hdfs-activate-component-hive-server2[10695]: + return 0
<13>Mar 20 19:55:56 google-dataproc-startup[10685]: <13>Mar 20 19:55:56 post-hdfs-activate-component-hive-server2[10695]: + local -r common_restart_drop_in=/etc/systemd/system/common/restart.conf
<13>Mar 20 19:55:56 google-dataproc-startup[10685]: <13>Mar 20 19:55:56 post-hdfs-activate-component-hive-server2[10695]: + [[ ! -f /etc/systemd/system/common/restart.conf ]]
<13>Mar 20 19:55:56 google-dataproc-startup[10685]: <13>Mar 20 19:55:56 post-hdfs-activate-component-hive-server2[10695]: + local -r worker_restart_drop_in=/etc/systemd/system/common/worker-restart.conf
<13>Mar 20 19:55:56 google-dataproc-startup[10685]: <13>Mar 20 19:55:56 post-hdfs-activate-component-hive-server2[10695]: + [[ ! -f /etc/systemd/system/common/worker-restart.conf ]]
<13>Mar 20 19:55:56 google-dataproc-startup[10685]: <13>Mar 20 19:55:56 post-hdfs-activate-component-hive-server2[10695]: + local -r drop_in_dir=/etc/systemd/system/hive-server2.service.d
<13>Mar 20 19:55:56 google-dataproc-startup[10685]: <13>Mar 20 19:55:56 post-hdfs-activate-component-hive-server2[10695]: + mkdir -p /etc/systemd/system/hive-server2.service.d
<13>Mar 20 19:55:56 google-dataproc-startup[10685]: <13>Mar 20 19:55:56 post-hdfs-activate-component-hive-server2[10695]: + local props
<13>Mar 20 19:55:56 google-dataproc-startup[10685]: <13>Mar 20 19:55:56 post-hdfs-activate-component-hive-server2[10695]: ++ systemctl show hive-server2.service -p Restart,RemainAfterExit
<13>Mar 20 19:55:56 google-dataproc-startup[10685]: <13>Mar 20 19:55:56 post-hdfs-activate-component-hive-server2[10695]: + props='Restart=no
<13>Mar 20 19:55:56 google-dataproc-startup[10685]: <13>Mar 20 19:55:56 post-hdfs-activate-component-hive-server2[10695]: RemainAfterExit=no'
<13>Mar 20 19:55:56 google-dataproc-startup[10685]: <13>Mar 20 19:55:56 post-hdfs-activate-component-hive-server2[10695]: + [[ hive-server2 != \h\a\d\o\o\p\-\h\d\f\s\-\d\a\t\a\n\o\d\e ]]
<13>Mar 20 19:55:56 google-dataproc-startup[10685]: <13>Mar 20 19:55:56 post-hdfs-activate-component-hive-server2[10695]: + [[ hive-server2 != \h\a\d\o\o\p\-\y\a\r\n\-\n\o\d\e\m\a\n\a\g\e\r ]]
<13>Mar 20 19:55:56 google-dataproc-startup[10685]: <13>Mar 20 19:55:56 post-hdfs-activate-component-hive-server2[10695]: + [[ Restart=no
<13>Mar 20 19:55:56 google-dataproc-startup[10685]: <13>Mar 20 19:55:56 post-hdfs-activate-component-hive-server2[10695]: RemainAfterExit=no == *\R\e\s\t\a\r\t\=\n\o* ]]
<13>Mar 20 19:55:56 google-dataproc-startup[10685]: <13>Mar 20 19:55:56 post-hdfs-activate-component-hive-server2[10695]: + [[ Restart=no
<13>Mar 20 19:55:56 google-dataproc-startup[10685]: <13>Mar 20 19:55:56 post-hdfs-activate-component-hive-server2[10695]: RemainAfterExit=no == *\R\e\m\a\i\n\A\f\t\e\r\E\x\i\t\=\n\o* ]]
<13>Mar 20 19:55:56 google-dataproc-startup[10685]: <13>Mar 20 19:55:56 post-hdfs-activate-component-hive-server2[10695]: + ln -s -f /etc/systemd/system/common/restart.conf /etc/systemd/system/hive-server2.service.d
<13>Mar 20 19:55:56 google-dataproc-startup[10685]: <13>Mar 20 19:55:56 post-hdfs-activate-component-hive-server2[10695]: + local -r gate_start_on_agent_success_drop_in=/etc/systemd/system/common/agent-gate.conf
<13>Mar 20 19:55:56 google-dataproc-startup[10685]: <13>Mar 20 19:55:56 post-hdfs-activate-component-hive-server2[10695]: + [[ ! -f /etc/systemd/system/common/agent-gate.conf ]]
<13>Mar 20 19:55:56 google-dataproc-startup[10685]: <13>Mar 20 19:55:56 post-hdfs-activate-component-hive-server2[10695]: + [[ hive-server2 == \h\a\d\o\o\p\-\h\d\f\s\-\d\a\t\a\n\o\d\e ]]
<13>Mar 20 19:55:56 google-dataproc-startup[10685]: <13>Mar 20 19:55:56 post-hdfs-activate-component-hive-server2[10695]: + [[ hive-server2 == \h\a\d\o\o\p\-\y\a\r\n\-\n\o\d\e\m\a\n\a\g\e\r ]]
<13>Mar 20 19:55:56 google-dataproc-startup[10685]: <13>Mar 20 19:55:56 post-hdfs-activate-component-hive-server2[10695]: + retry_constant systemctl start hive-server2
<13>Mar 20 19:55:56 google-dataproc-startup[10685]: <13>Mar 20 19:55:56 post-hdfs-activate-component-hive-server2[10695]: + retry_constant_custom 300 1 systemctl start hive-server2
<13>Mar 20 19:55:56 google-dataproc-startup[10685]: <13>Mar 20 19:55:56 post-hdfs-activate-component-hive-server2[10695]: + local -r max_retry_time=300
<13>Mar 20 19:55:56 google-dataproc-startup[10685]: <13>Mar 20 19:55:56 post-hdfs-activate-component-hive-server2[10695]: + local -r retry_delay=1
<13>Mar 20 19:55:56 google-dataproc-startup[10685]: <13>Mar 20 19:55:56 post-hdfs-activate-component-hive-server2[10695]: + cmd=("${@:3}")
<13>Mar 20 19:55:56 google-dataproc-startup[10685]: <13>Mar 20 19:55:56 post-hdfs-activate-component-hive-server2[10695]: + local -r cmd
<13>Mar 20 19:55:56 google-dataproc-startup[10685]: <13>Mar 20 19:55:56 post-hdfs-activate-component-hive-server2[10695]: + local -r max_retries=300
<13>Mar 20 19:55:56 google-dataproc-startup[10685]: <13>Mar 20 19:55:56 post-hdfs-activate-component-hive-server2[10695]: + set +x
<13>Mar 20 19:55:56 google-dataproc-startup[10685]: <13>Mar 20 19:55:56 post-hdfs-activate-component-hive-server2[10695]: About to run 'systemctl start hive-server2' with retries...
<13>Mar 20 19:55:56 google-dataproc-startup[10685]: <13>Mar 20 19:55:56 post-hdfs-activate-component-hive-server2[10695]: Warning: The unit file, source configuration file or drop-ins of hive-server2.service changed on disk. Run 'systemctl daemon-reload' to reload units.
<13>Mar 20 19:56:00 google-dataproc-startup[10685]: <13>Mar 20 19:56:00 post-hdfs-activate-component-hive-server2[10695]: systemctl start hive-server2 succeeded.
<13>Mar 20 19:56:00 google-dataproc-startup[10685]: <13>Mar 20 19:56:00 post-hdfs-activate-component-hive-server2[10695]: + return 0
<13>Mar 20 19:56:00 google-dataproc-startup[10685]: <13>Mar 20 19:56:00 post-hdfs-activate-component-hive-server2[10695]: + local thrift_port
<13>Mar 20 19:56:00 google-dataproc-startup[10685]: <13>Mar 20 19:56:00 post-hdfs-activate-component-hive-server2[10695]: ++ get_property_in_xml /etc/hive/conf/hive-site.xml hive.server2.thrift.port 10000
<13>Mar 20 19:56:00 google-dataproc-startup[10685]: <13>Mar 20 19:56:00 post-hdfs-activate-component-hive-server2[10695]: ++ local -r path=/etc/hive/conf/hive-site.xml
<13>Mar 20 19:56:00 google-dataproc-startup[10685]: <13>Mar 20 19:56:00 post-hdfs-activate-component-hive-server2[10695]: ++ local -r property=hive.server2.thrift.port
<13>Mar 20 19:56:00 google-dataproc-startup[10685]: <13>Mar 20 19:56:00 post-hdfs-activate-component-hive-server2[10695]: ++ local -r default_value=10000
<13>Mar 20 19:56:00 google-dataproc-startup[10685]: <13>Mar 20 19:56:00 post-hdfs-activate-component-hive-server2[10695]: ++ local val
<13>Mar 20 19:56:00 google-dataproc-startup[10685]: <13>Mar 20 19:56:00 post-hdfs-activate-component-hive-server2[10695]: +++ bdconfig get_property_value --configuration_file /etc/hive/conf/hive-site.xml --name hive.server2.thrift.port
<13>Mar 20 19:56:00 google-dataproc-startup[10685]: <13>Mar 20 19:56:00 post-hdfs-activate-component-spark[10699]: <13>Mar 20 19:56:00 post-hdfs-component-spark[10724]: systemctl start spark-history-server succeeded.
<13>Mar 20 19:56:00 google-dataproc-startup[10685]: <13>Mar 20 19:56:00 post-hdfs-activate-component-spark[10699]: <13>Mar 20 19:56:00 post-hdfs-component-spark[10724]: + return 0
<13>Mar 20 19:56:00 google-dataproc-startup[10685]: <13>Mar 20 19:56:00 post-hdfs-activate-component-spark[10699]: + [[ 0 -ne 0 ]]
<13>Mar 20 19:56:00 google-dataproc-startup[10685]: <13>Mar 20 19:56:00 post-hdfs-activate-component-spark[10699]: + touch /tmp/dataproc/components/post-hdfs/spark.done
<13>Mar 20 19:56:00 google-dataproc-startup[10685]: + (( status != 0 ))
<13>Mar 20 19:56:00 google-dataproc-startup[10685]: + tee /tmp/dataproc/commands/10699.done
<13>Mar 20 19:56:00 google-dataproc-startup[10685]: + echo 'Command cmd=[post_hdfs_activate_component spark] pid=10699 exited with 0'
<13>Mar 20 19:56:00 google-dataproc-startup[10685]: Command cmd=[post_hdfs_activate_component spark] pid=10699 exited with 0
<13>Mar 20 19:56:00 google-dataproc-startup[10685]: + (( ++i ))
<13>Mar 20 19:56:00 google-dataproc-startup[10685]: + (( i < 8 ))
<13>Mar 20 19:56:00 google-dataproc-startup[10685]: + local pid=10698
<13>Mar 20 19:56:00 google-dataproc-startup[10685]: + local 'cmd=post_hdfs_activate_component mysql'
<13>Mar 20 19:56:00 google-dataproc-startup[10685]: + loginfo 'Waiting on pid=10698 cmd=[post_hdfs_activate_component mysql]'
<13>Mar 20 19:56:00 google-dataproc-startup[10685]: + echo 'Waiting on pid=10698 cmd=[post_hdfs_activate_component mysql]'
<13>Mar 20 19:56:00 google-dataproc-startup[10685]: Waiting on pid=10698 cmd=[post_hdfs_activate_component mysql]
<13>Mar 20 19:56:00 google-dataproc-startup[10685]: + echo 'post_hdfs_activate_component mysql'
<13>Mar 20 19:56:00 google-dataproc-startup[10685]: + local status=0
<13>Mar 20 19:56:00 google-dataproc-startup[10685]: + wait 10698
<13>Mar 20 19:56:00 google-dataproc-startup[10685]: + (( status != 0 ))
<13>Mar 20 19:56:00 google-dataproc-startup[10685]: + tee /tmp/dataproc/commands/10698.done
<13>Mar 20 19:56:00 google-dataproc-startup[10685]: + echo 'Command cmd=[post_hdfs_activate_component mysql] pid=10698 exited with 0'
<13>Mar 20 19:56:00 google-dataproc-startup[10685]: Command cmd=[post_hdfs_activate_component mysql] pid=10698 exited with 0
<13>Mar 20 19:56:00 google-dataproc-startup[10685]: + (( ++i ))
<13>Mar 20 19:56:00 google-dataproc-startup[10685]: + (( i < 8 ))
<13>Mar 20 19:56:00 google-dataproc-startup[10685]: + local pid=10697
<13>Mar 20 19:56:00 google-dataproc-startup[10685]: + local 'cmd=post_hdfs_activate_component miniconda3'
<13>Mar 20 19:56:00 google-dataproc-startup[10685]: + loginfo 'Waiting on pid=10697 cmd=[post_hdfs_activate_component miniconda3]'
<13>Mar 20 19:56:00 google-dataproc-startup[10685]: + echo 'Waiting on pid=10697 cmd=[post_hdfs_activate_component miniconda3]'
<13>Mar 20 19:56:00 google-dataproc-startup[10685]: Waiting on pid=10697 cmd=[post_hdfs_activate_component miniconda3]
<13>Mar 20 19:56:00 google-dataproc-startup[10685]: + echo 'post_hdfs_activate_component miniconda3'
<13>Mar 20 19:56:00 google-dataproc-startup[10685]: + local status=0
<13>Mar 20 19:56:00 google-dataproc-startup[10685]: + wait 10697
<13>Mar 20 19:56:00 google-dataproc-startup[10685]: + (( status != 0 ))
<13>Mar 20 19:56:00 google-dataproc-startup[10685]: + tee /tmp/dataproc/commands/10697.done
<13>Mar 20 19:56:00 google-dataproc-startup[10685]: + echo 'Command cmd=[post_hdfs_activate_component miniconda3] pid=10697 exited with 0'
<13>Mar 20 19:56:00 google-dataproc-startup[10685]: Command cmd=[post_hdfs_activate_component miniconda3] pid=10697 exited with 0
<13>Mar 20 19:56:00 google-dataproc-startup[10685]: + (( ++i ))
<13>Mar 20 19:56:00 google-dataproc-startup[10685]: + (( i < 8 ))
<13>Mar 20 19:56:00 google-dataproc-startup[10685]: + local pid=10696
<13>Mar 20 19:56:00 google-dataproc-startup[10685]: + local 'cmd=post_hdfs_activate_component mapreduce'
<13>Mar 20 19:56:00 google-dataproc-startup[10685]: + loginfo 'Waiting on pid=10696 cmd=[post_hdfs_activate_component mapreduce]'
<13>Mar 20 19:56:00 google-dataproc-startup[10685]: + echo 'Waiting on pid=10696 cmd=[post_hdfs_activate_component mapreduce]'
<13>Mar 20 19:56:00 google-dataproc-startup[10685]: Waiting on pid=10696 cmd=[post_hdfs_activate_component mapreduce]
<13>Mar 20 19:56:00 google-dataproc-startup[10685]: + echo 'post_hdfs_activate_component mapreduce'
<13>Mar 20 19:56:00 google-dataproc-startup[10685]: + local status=0
<13>Mar 20 19:56:00 google-dataproc-startup[10685]: + wait 10696
<13>Mar 20 19:56:00 google-dataproc-startup[10685]: + (( status != 0 ))
<13>Mar 20 19:56:00 google-dataproc-startup[10685]: + tee /tmp/dataproc/commands/10696.done
<13>Mar 20 19:56:00 google-dataproc-startup[10685]: + echo 'Command cmd=[post_hdfs_activate_component mapreduce] pid=10696 exited with 0'
<13>Mar 20 19:56:00 google-dataproc-startup[10685]: Command cmd=[post_hdfs_activate_component mapreduce] pid=10696 exited with 0
<13>Mar 20 19:56:00 google-dataproc-startup[10685]: + (( ++i ))
<13>Mar 20 19:56:00 google-dataproc-startup[10685]: + (( i < 8 ))
<13>Mar 20 19:56:00 google-dataproc-startup[10685]: + local pid=10695
<13>Mar 20 19:56:00 google-dataproc-startup[10685]: + local 'cmd=post_hdfs_activate_component hive-server2'
<13>Mar 20 19:56:00 google-dataproc-startup[10685]: + loginfo 'Waiting on pid=10695 cmd=[post_hdfs_activate_component hive-server2]'
<13>Mar 20 19:56:00 google-dataproc-startup[10685]: + echo 'Waiting on pid=10695 cmd=[post_hdfs_activate_component hive-server2]'
<13>Mar 20 19:56:00 google-dataproc-startup[10685]: Waiting on pid=10695 cmd=[post_hdfs_activate_component hive-server2]
<13>Mar 20 19:56:00 google-dataproc-startup[10685]: + echo 'post_hdfs_activate_component hive-server2'
<13>Mar 20 19:56:00 google-dataproc-startup[10685]: + local status=0
<13>Mar 20 19:56:00 google-dataproc-startup[10685]: + wait 10695
<13>Mar 20 19:56:00 google-dataproc-startup[10685]: <13>Mar 20 19:56:00 post-hdfs-activate-component-hive-server2[10695]: ++ val=None
<13>Mar 20 19:56:00 google-dataproc-startup[10685]: <13>Mar 20 19:56:00 post-hdfs-activate-component-hive-server2[10695]: ++ [[ None == \N\o\n\e ]]
<13>Mar 20 19:56:00 google-dataproc-startup[10685]: <13>Mar 20 19:56:00 post-hdfs-activate-component-hive-server2[10695]: ++ val=10000
<13>Mar 20 19:56:00 google-dataproc-startup[10685]: <13>Mar 20 19:56:00 post-hdfs-activate-component-hive-server2[10695]: ++ echo 10000
<13>Mar 20 19:56:00 google-dataproc-startup[10685]: <13>Mar 20 19:56:00 post-hdfs-activate-component-hive-server2[10695]: + thrift_port=10000
<13>Mar 20 19:56:00 google-dataproc-startup[10685]: <13>Mar 20 19:56:00 post-hdfs-activate-component-hive-server2[10695]: + local timeout
<13>Mar 20 19:56:00 google-dataproc-startup[10685]: <13>Mar 20 19:56:00 post-hdfs-activate-component-hive-server2[10695]: ++ get_dataproc_property_or_default startup.component.service-binding-timeout.hive-server2 300
<13>Mar 20 19:56:00 google-dataproc-startup[10685]: <13>Mar 20 19:56:00 post-hdfs-activate-component-hive-server2[10695]: ++ local -r property_name=startup.component.service-binding-timeout.hive-server2
<13>Mar 20 19:56:00 google-dataproc-startup[10685]: <13>Mar 20 19:56:00 post-hdfs-activate-component-hive-server2[10695]: ++ local -r default_value=300
<13>Mar 20 19:56:00 google-dataproc-startup[10685]: <13>Mar 20 19:56:00 post-hdfs-activate-component-hive-server2[10695]: ++ local actual_value
<13>Mar 20 19:56:00 google-dataproc-startup[10685]: <13>Mar 20 19:56:00 post-hdfs-activate-component-hive-server2[10695]: +++ get_dataproc_property startup.component.service-binding-timeout.hive-server2
<13>Mar 20 19:56:00 google-dataproc-startup[10685]: <13>Mar 20 19:56:00 post-hdfs-activate-component-hive-server2[10695]: +++ local -r property_name=startup.component.service-binding-timeout.hive-server2
<13>Mar 20 19:56:00 google-dataproc-startup[10685]: <13>Mar 20 19:56:00 post-hdfs-activate-component-hive-server2[10695]: +++ local property_value
<13>Mar 20 19:56:00 google-dataproc-startup[10685]: <13>Mar 20 19:56:00 post-hdfs-activate-component-hive-server2[10695]: ++++ get_java_property /etc/google-dataproc/dataproc.properties startup.component.service-binding-timeout.hive-server2
<13>Mar 20 19:56:00 google-dataproc-startup[10685]: <13>Mar 20 19:56:00 post-hdfs-activate-component-hive-server2[10695]: ++++ local -r property_file=/etc/google-dataproc/dataproc.properties
<13>Mar 20 19:56:00 google-dataproc-startup[10685]: <13>Mar 20 19:56:00 post-hdfs-activate-component-hive-server2[10695]: ++++ local -r property_name=startup.component.service-binding-timeout.hive-server2
<13>Mar 20 19:56:00 google-dataproc-startup[10685]: <13>Mar 20 19:56:00 post-hdfs-activate-component-hive-server2[10695]: ++++ local property_value
<13>Mar 20 19:56:00 google-dataproc-startup[10685]: <13>Mar 20 19:56:00 post-hdfs-activate-component-hive-server2[10695]: +++++ sed -r 's/\\([#!=:])/\1/g'
<13>Mar 20 19:56:00 google-dataproc-startup[10685]: <13>Mar 20 19:56:00 post-hdfs-activate-component-hive-server2[10695]: +++++ grep '^startup.component.service-binding-timeout.hive-server2=' /etc/google-dataproc/dataproc.properties
<13>Mar 20 19:56:00 google-dataproc-startup[10685]: <13>Mar 20 19:56:00 post-hdfs-activate-component-hive-server2[10695]: +++++ cut -d = -f 2-
<13>Mar 20 19:56:00 google-dataproc-startup[10685]: <13>Mar 20 19:56:00 post-hdfs-activate-component-hive-server2[10695]: +++++ tail -n 1
<13>Mar 20 19:56:00 google-dataproc-startup[10685]: <13>Mar 20 19:56:00 post-hdfs-activate-component-hive-server2[10695]: ++++ property_value=
<13>Mar 20 19:56:00 google-dataproc-startup[10685]: <13>Mar 20 19:56:00 post-hdfs-activate-component-hive-server2[10695]: ++++ echo ''
<13>Mar 20 19:56:00 google-dataproc-startup[10685]: <13>Mar 20 19:56:00 post-hdfs-activate-component-hive-server2[10695]: +++ property_value=
<13>Mar 20 19:56:00 google-dataproc-startup[10685]: <13>Mar 20 19:56:00 post-hdfs-activate-component-hive-server2[10695]: +++ echo ''
<13>Mar 20 19:56:00 google-dataproc-startup[10685]: <13>Mar 20 19:56:00 post-hdfs-activate-component-hive-server2[10695]: ++ actual_value=
<13>Mar 20 19:56:00 google-dataproc-startup[10685]: <13>Mar 20 19:56:00 post-hdfs-activate-component-hive-server2[10695]: ++ [[ -n '' ]]
<13>Mar 20 19:56:00 google-dataproc-startup[10685]: <13>Mar 20 19:56:00 post-hdfs-activate-component-hive-server2[10695]: ++ echo 300
<13>Mar 20 19:56:00 google-dataproc-startup[10685]: <13>Mar 20 19:56:00 post-hdfs-activate-component-hive-server2[10695]: + timeout=300
<13>Mar 20 19:56:00 google-dataproc-startup[10685]: <13>Mar 20 19:56:00 post-hdfs-activate-component-hive-server2[10695]: + wait_for_port hive-server2 tianzhenyv-qwiklab-m 10000 300
<13>Mar 20 19:56:00 google-dataproc-startup[10685]: <13>Mar 20 19:56:00 post-hdfs-activate-component-hive-server2[10695]: + local -r name=hive-server2
<13>Mar 20 19:56:00 google-dataproc-startup[10685]: <13>Mar 20 19:56:00 post-hdfs-activate-component-hive-server2[10695]: + local -r host=tianzhenyv-qwiklab-m
<13>Mar 20 19:56:00 google-dataproc-startup[10685]: <13>Mar 20 19:56:00 post-hdfs-activate-component-hive-server2[10695]: + local -r port=10000
<13>Mar 20 19:56:00 google-dataproc-startup[10685]: <13>Mar 20 19:56:00 post-hdfs-activate-component-hive-server2[10695]: + local -r timeout=300
<13>Mar 20 19:56:00 google-dataproc-startup[10685]: <13>Mar 20 19:56:00 post-hdfs-activate-component-hive-server2[10695]: + local -r capped_timeout=300
<13>Mar 20 19:56:00 google-dataproc-startup[10685]: <13>Mar 20 19:56:00 post-hdfs-activate-component-hive-server2[10695]: + loginfo 'Waiting 300 seconds for service to come up on host=tianzhenyv-qwiklab-m port=10000 name=hive-server2.'
<13>Mar 20 19:56:00 google-dataproc-startup[10685]: <13>Mar 20 19:56:00 post-hdfs-activate-component-hive-server2[10695]: + echo 'Waiting 300 seconds for service to come up on host=tianzhenyv-qwiklab-m port=10000 name=hive-server2.'
<13>Mar 20 19:56:00 google-dataproc-startup[10685]: <13>Mar 20 19:56:00 post-hdfs-activate-component-hive-server2[10695]: Waiting 300 seconds for service to come up on host=tianzhenyv-qwiklab-m port=10000 name=hive-server2.
<13>Mar 20 19:56:00 google-dataproc-startup[10685]: <13>Mar 20 19:56:00 post-hdfs-activate-component-hive-server2[10695]: + retry_constant_custom 300 1 nc -v -z -w 1 tianzhenyv-qwiklab-m 10000
<13>Mar 20 19:56:00 google-dataproc-startup[10685]: <13>Mar 20 19:56:00 post-hdfs-activate-component-hive-server2[10695]: + local -r max_retry_time=300
<13>Mar 20 19:56:00 google-dataproc-startup[10685]: <13>Mar 20 19:56:00 post-hdfs-activate-component-hive-server2[10695]: + local -r retry_delay=1
<13>Mar 20 19:56:00 google-dataproc-startup[10685]: <13>Mar 20 19:56:00 post-hdfs-activate-component-hive-server2[10695]: + cmd=("${@:3}")
<13>Mar 20 19:56:00 google-dataproc-startup[10685]: <13>Mar 20 19:56:00 post-hdfs-activate-component-hive-server2[10695]: + local -r cmd
<13>Mar 20 19:56:00 google-dataproc-startup[10685]: <13>Mar 20 19:56:00 post-hdfs-activate-component-hive-server2[10695]: + local -r max_retries=300
<13>Mar 20 19:56:00 google-dataproc-startup[10685]: <13>Mar 20 19:56:00 post-hdfs-activate-component-hive-server2[10695]: + set +x
<13>Mar 20 19:56:00 google-dataproc-startup[10685]: <13>Mar 20 19:56:00 post-hdfs-activate-component-hive-server2[10695]: About to run 'nc -v -z -w 1 tianzhenyv-qwiklab-m 10000' with retries...
<13>Mar 20 19:56:00 google-dataproc-startup[10685]: <13>Mar 20 19:56:00 post-hdfs-activate-component-hive-server2[10695]: nc: connect to tianzhenyv-qwiklab-m port 10000 (tcp) failed: Connection refused
<13>Mar 20 19:56:00 google-dataproc-startup[10685]: <13>Mar 20 19:56:00 post-hdfs-activate-component-hive-server2[10695]: 'nc -v -z -w 1 tianzhenyv-qwiklab-m 10000' attempt 1 failed! Sleeping 1s.
<13>Mar 20 19:56:01 google-dataproc-startup[10685]: <13>Mar 20 19:56:01 post-hdfs-activate-component-hive-server2[10695]: nc: connect to tianzhenyv-qwiklab-m port 10000 (tcp) failed: Connection refused
<13>Mar 20 19:56:02 google-dataproc-startup[10685]: <13>Mar 20 19:56:02 post-hdfs-activate-component-hive-server2[10695]: nc: connect to tianzhenyv-qwiklab-m port 10000 (tcp) failed: Connection refused
<13>Mar 20 19:56:03 google-dataproc-startup[10685]: <13>Mar 20 19:56:03 post-hdfs-activate-component-hive-server2[10695]: nc: connect to tianzhenyv-qwiklab-m port 10000 (tcp) failed: Connection refused
<13>Mar 20 19:56:04 google-dataproc-startup[10685]: <13>Mar 20 19:56:04 post-hdfs-activate-component-hive-server2[10695]: nc: connect to tianzhenyv-qwiklab-m port 10000 (tcp) failed: Connection refused
<13>Mar 20 19:56:05 google-dataproc-startup[10685]: <13>Mar 20 19:56:05 post-hdfs-activate-component-hive-server2[10695]: nc: connect to tianzhenyv-qwiklab-m port 10000 (tcp) failed: Connection refused
<13>Mar 20 19:56:06 google-dataproc-startup[10685]: <13>Mar 20 19:56:06 post-hdfs-activate-component-hive-server2[10695]: nc: connect to tianzhenyv-qwiklab-m port 10000 (tcp) failed: Connection refused
<13>Mar 20 19:56:07 google-dataproc-startup[10685]: <13>Mar 20 19:56:07 post-hdfs-activate-component-hive-server2[10695]: nc: connect to tianzhenyv-qwiklab-m port 10000 (tcp) failed: Connection refused
<13>Mar 20 19:56:08 google-dataproc-startup[10685]: <13>Mar 20 19:56:08 post-hdfs-activate-component-hive-server2[10695]: nc: connect to tianzhenyv-qwiklab-m port 10000 (tcp) failed: Connection refused
<13>Mar 20 19:56:09 google-dataproc-startup[10685]: <13>Mar 20 19:56:09 post-hdfs-activate-component-hive-server2[10695]: nc: connect to tianzhenyv-qwiklab-m port 10000 (tcp) failed: Connection refused
<13>Mar 20 19:56:10 google-dataproc-startup[10685]: <13>Mar 20 19:56:10 post-hdfs-activate-component-hive-server2[10695]: Connection to tianzhenyv-qwiklab-m 10000 port [tcp/webmin] succeeded!
<13>Mar 20 19:56:10 google-dataproc-startup[10685]: <13>Mar 20 19:56:10 post-hdfs-activate-component-hive-server2[10695]: nc -v -z -w 1 tianzhenyv-qwiklab-m 10000 succeeded.
<13>Mar 20 19:56:10 google-dataproc-startup[10685]: <13>Mar 20 19:56:10 post-hdfs-activate-component-hive-server2[10695]: + return 0
<13>Mar 20 19:56:10 google-dataproc-startup[10685]: <13>Mar 20 19:56:10 post-hdfs-activate-component-hive-server2[10695]: + loginfo 'Service up on host=tianzhenyv-qwiklab-m port=10000 name=hive-server2.'
<13>Mar 20 19:56:10 google-dataproc-startup[10685]: <13>Mar 20 19:56:10 post-hdfs-activate-component-hive-server2[10695]: + echo 'Service up on host=tianzhenyv-qwiklab-m port=10000 name=hive-server2.'
<13>Mar 20 19:56:10 google-dataproc-startup[10685]: <13>Mar 20 19:56:10 post-hdfs-activate-component-hive-server2[10695]: Service up on host=tianzhenyv-qwiklab-m port=10000 name=hive-server2.
<13>Mar 20 19:56:10 google-dataproc-startup[10685]: <13>Mar 20 19:56:10 post-hdfs-activate-component-hive-server2[10695]: + [[ 0 -ne 0 ]]
<13>Mar 20 19:56:10 google-dataproc-startup[10685]: <13>Mar 20 19:56:10 post-hdfs-activate-component-hive-server2[10695]: + touch /tmp/dataproc/components/post-hdfs/hive-server2.done
<13>Mar 20 19:56:10 google-dataproc-startup[10685]: + (( status != 0 ))
<13>Mar 20 19:56:10 google-dataproc-startup[10685]: + tee /tmp/dataproc/commands/10695.done
<13>Mar 20 19:56:10 google-dataproc-startup[10685]: + echo 'Command cmd=[post_hdfs_activate_component hive-server2] pid=10695 exited with 0'
<13>Mar 20 19:56:10 google-dataproc-startup[10685]: Command cmd=[post_hdfs_activate_component hive-server2] pid=10695 exited with 0
<13>Mar 20 19:56:10 google-dataproc-startup[10685]: + (( ++i ))
<13>Mar 20 19:56:10 google-dataproc-startup[10685]: + (( i < 8 ))
<13>Mar 20 19:56:10 google-dataproc-startup[10685]: + local pid=10694
<13>Mar 20 19:56:10 google-dataproc-startup[10685]: + local 'cmd=post_hdfs_activate_component hive-metastore'
<13>Mar 20 19:56:10 google-dataproc-startup[10685]: + loginfo 'Waiting on pid=10694 cmd=[post_hdfs_activate_component hive-metastore]'
<13>Mar 20 19:56:10 google-dataproc-startup[10685]: + echo 'Waiting on pid=10694 cmd=[post_hdfs_activate_component hive-metastore]'
<13>Mar 20 19:56:10 google-dataproc-startup[10685]: Waiting on pid=10694 cmd=[post_hdfs_activate_component hive-metastore]
<13>Mar 20 19:56:10 google-dataproc-startup[10685]: + echo 'post_hdfs_activate_component hive-metastore'
<13>Mar 20 19:56:10 google-dataproc-startup[10685]: + local status=0
<13>Mar 20 19:56:10 google-dataproc-startup[10685]: + wait 10694
<13>Mar 20 19:56:10 google-dataproc-startup[10685]: + (( status != 0 ))
<13>Mar 20 19:56:10 google-dataproc-startup[10685]: + tee /tmp/dataproc/commands/10694.done
<13>Mar 20 19:56:10 google-dataproc-startup[10685]: + echo 'Command cmd=[post_hdfs_activate_component hive-metastore] pid=10694 exited with 0'
<13>Mar 20 19:56:10 google-dataproc-startup[10685]: Command cmd=[post_hdfs_activate_component hive-metastore] pid=10694 exited with 0
<13>Mar 20 19:56:10 google-dataproc-startup[10685]: + (( ++i ))
<13>Mar 20 19:56:10 google-dataproc-startup[10685]: + (( i < 8 ))
<13>Mar 20 19:56:10 google-dataproc-startup[10685]: + local pid=10693
<13>Mar 20 19:56:10 google-dataproc-startup[10685]: + local 'cmd=post_hdfs_activate_component hdfs'
<13>Mar 20 19:56:10 google-dataproc-startup[10685]: + loginfo 'Waiting on pid=10693 cmd=[post_hdfs_activate_component hdfs]'
<13>Mar 20 19:56:10 google-dataproc-startup[10685]: + echo 'Waiting on pid=10693 cmd=[post_hdfs_activate_component hdfs]'
<13>Mar 20 19:56:10 google-dataproc-startup[10685]: Waiting on pid=10693 cmd=[post_hdfs_activate_component hdfs]
<13>Mar 20 19:56:10 google-dataproc-startup[10685]: + echo 'post_hdfs_activate_component hdfs'
<13>Mar 20 19:56:10 google-dataproc-startup[10685]: + local status=0
<13>Mar 20 19:56:10 google-dataproc-startup[10685]: + wait 10693
<13>Mar 20 19:56:10 google-dataproc-startup[10685]: + (( status != 0 ))
<13>Mar 20 19:56:10 google-dataproc-startup[10685]: + echo 'Command cmd=[post_hdfs_activate_component hdfs] pid=10693 exited with 0'
<13>Mar 20 19:56:10 google-dataproc-startup[10685]: + tee /tmp/dataproc/commands/10693.done
<13>Mar 20 19:56:10 google-dataproc-startup[10685]: Command cmd=[post_hdfs_activate_component hdfs] pid=10693 exited with 0
<13>Mar 20 19:56:10 google-dataproc-startup[10685]: + (( ++i ))
<13>Mar 20 19:56:10 google-dataproc-startup[10685]: + (( i < 8 ))
<13>Mar 20 19:56:10 google-dataproc-startup[10685]: + BACKGROUND_PROCESSES=()
<13>Mar 20 19:56:10 google-dataproc-startup[10685]: + BACKGROUND_COMMANDS=()
<13>Mar 20 19:56:10 google-dataproc-startup[10685]: + loginfo 'All done'
<13>Mar 20 19:56:10 google-dataproc-startup[10685]: + echo 'All done'
<13>Mar 20 19:56:10 google-dataproc-startup[10685]: All done
